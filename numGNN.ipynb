{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08d8716-5267-467e-8fbf-f956bfbeeaca",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38601030-b6e2-4ea2-9844-49c943334f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GINConv, GINEConv, GCNConv, GraphConv, SAGEConv, ChebConv, SAGPooling, global_add_pool, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils.utils import generate_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b376f8b-82c5-40bb-ba48-0dedd85f7591",
   "metadata": {},
   "source": [
    "# Run Pytorch on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f424459d-d858-4255-9250-1f940079f474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_pytorch_version(version):\n",
    "    return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "    return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34ea2e-7a10-4e27-8fe3-29fb7b0132a6",
   "metadata": {},
   "source": [
    "# DataSet & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac311ce8-9e84-4c67-8d4e-2d8f8659cbf2",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6066e87-d49c-4207-8c46-cb7983177cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topo_0                                           0\n",
      "topo_1                                           0\n",
      "topo_2                                           0\n",
      "topo_3                                           0\n",
      "topo_4                                           0\n",
      "topo_5                                           0\n",
      "topo_6                                           0\n",
      "topo_7                                           0\n",
      "topo_8                                           0\n",
      "topo_9                                           0\n",
      "MOFname                                          0\n",
      "volume [A^3]                                     0\n",
      "weight [u]                                       0\n",
      "density [g/cm^3]                                 0\n",
      "surface_area [m^2/g]                             0\n",
      "void_fraction                                    0\n",
      "void_volume [cm^3/g]                             0\n",
      "functional_groups                                0\n",
      "metal_linker                                     0\n",
      "organic_linker1                                  0\n",
      "organic_linker2                                  0\n",
      "catalog CO2/N2                                   0\n",
      "CO2/N2_selectivity                               0\n",
      "heat_adsorption_CO2_P0.15bar_T298K [kcal/mol]    0\n",
      "CO2_working_capacity [mL/g]                      0\n",
      "Smiles                                           0\n",
      "dtype: int64\n",
      "(68611, 26)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train/clean_train_edited.csv')\n",
    "smiles = pd.read_csv('data/train/smiles_train.csv')\n",
    "data = df.join(smiles.set_index('MOFname'), on='MOFname')\n",
    "\n",
    "data = data.dropna(subset=['Smiles'])\n",
    "data = data.reset_index(drop=True)\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "print(data.isnull().sum())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11f0e34-b17a-4f43-9a1a-0b4f0d2fc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map = {\n",
    "    'atomic_num':\n",
    "    list(range(0, 119)),\n",
    "    'chirality': [\n",
    "        'CHI_UNSPECIFIED',\n",
    "        'CHI_TETRAHEDRAL_CW',\n",
    "        'CHI_TETRAHEDRAL_CCW',\n",
    "        'CHI_OTHER',\n",
    "    ],\n",
    "    'degree':\n",
    "    list(range(0, 11)),\n",
    "    'formal_charge':\n",
    "    list(range(-5, 7)),\n",
    "    'num_hs':\n",
    "    list(range(0, 9)),\n",
    "    'num_radical_electrons':\n",
    "    list(range(0, 5)),\n",
    "    'hybridization': [\n",
    "        'UNSPECIFIED',\n",
    "        'S',\n",
    "        'SP',\n",
    "        'SP2',\n",
    "        'SP3',\n",
    "        'SP3D',\n",
    "        'SP3D2',\n",
    "        'OTHER',\n",
    "    ],\n",
    "    'is_aromatic': [False, True],\n",
    "    'is_in_ring': [False, True],\n",
    "}\n",
    "\n",
    "e_map = {\n",
    "    'bond_type': [\n",
    "        'misc',\n",
    "        'SINGLE',\n",
    "        'DOUBLE',\n",
    "        'TRIPLE',\n",
    "        'AROMATIC',\n",
    "    ],\n",
    "    'stereo': [\n",
    "        'STEREONONE',\n",
    "        'STEREOZ',\n",
    "        'STEREOE',\n",
    "        'STEREOCIS',\n",
    "        'STEREOTRANS',\n",
    "        'STEREOANY',\n",
    "    ],\n",
    "    'is_conjugated': [False, True],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc54ebf3-b60c-40b5-97cc-2d12fb058556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 10000\n",
      "done: 20000\n",
      "done: 30000\n",
      "done: 40000\n",
      "done: 50000\n",
      "done: 60000\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "data_dict = []\n",
    "c = 1\n",
    "for _, line in data.iterrows():\n",
    "    mol = Chem.MolFromSmiles(line['Smiles'])\n",
    "    \n",
    "    if mol == None:\n",
    "        continue\n",
    "    \n",
    "    # Create Node Features\n",
    "    xs = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        x = []\n",
    "        x.append(x_map['atomic_num'].index(atom.GetAtomicNum()))\n",
    "        x.append(x_map['chirality'].index(str(atom.GetChiralTag())))\n",
    "        x.append(x_map['degree'].index(atom.GetTotalDegree()))\n",
    "        x.append(x_map['formal_charge'].index(atom.GetFormalCharge()))\n",
    "        x.append(x_map['num_hs'].index(atom.GetTotalNumHs()))\n",
    "        x.append(x_map['num_radical_electrons'].index(atom.GetNumRadicalElectrons()))\n",
    "        x.append(x_map['hybridization'].index(str(atom.GetHybridization())))\n",
    "        x.append(x_map['is_aromatic'].index(atom.GetIsAromatic()))\n",
    "        x.append(x_map['is_in_ring'].index(atom.IsInRing()))\n",
    "        xs.append(x)\n",
    "    x = torch.tensor(xs, dtype=torch.float).view(-1, 9)\n",
    "    \n",
    "    # Create Edge Features\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        e = []\n",
    "        e.append(e_map['bond_type'].index(str(bond.GetBondType())))\n",
    "        e.append(e_map['stereo'].index(str(bond.GetStereo())))\n",
    "        e.append(e_map['is_conjugated'].index(bond.GetIsConjugated()))\n",
    "\n",
    "        edge_indices += [[i, j], [j, i]]\n",
    "        edge_attrs += [e, e]\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices)\n",
    "    edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3)\n",
    "\n",
    "    # Sort indices.\n",
    "    if edge_index.numel() > 0:\n",
    "        perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n",
    "        edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n",
    "\n",
    "    x_feat = line.drop(['MOFname', 'weight [u]', 'functional_groups', 'CO2_working_capacity [mL/g]', 'Smiles']).values.astype(float) #, 'weight [u]', 'functional_groups', 'CO2_working_capacity [mL/g]'\n",
    "    x_feat = np.expand_dims(x_feat, axis=0)\n",
    "    x_feat = torch.tensor(x_feat)\n",
    "    y=torch.tensor([line['CO2_working_capacity [mL/g]']], dtype=torch.float).view(1, -1)\n",
    "    data_d = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, smiles=line['Smiles'], mofname=line['MOFname'], x_feat=x_feat) #, y=y\n",
    "    data_d.num_nodes = len(mol.GetAtoms())\n",
    "    data_list.append(data_d)\n",
    "    data_dict.append(line['MOFname'])\n",
    "    \n",
    "    if(c%10000==0):\n",
    "        print('done:',c)\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94bd46-dbbc-4b35-8136-4cbdf8b6630d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0214f3-676a-4117-a494-237cab095356",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = pickle.load(open('data/train/graph_concat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e88aa4-e0cf-42cb-917c-f666281c8f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 55125\n",
      "Number of test graphs: 10500\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "datasets = data_list\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(datasets, test_size=0.16, random_state = 1, shuffle=True)\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c803d206-82dc-46b1-a86a-3dfbb2364d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = []\n",
    "test_feat = []\n",
    "\n",
    "for train_data in train_dataset:\n",
    "    train_feat.append(train_data.x_feat)\n",
    "    \n",
    "for test_data in test_dataset:\n",
    "    test_feat.append(test_data.x_feat)\n",
    "\n",
    "\n",
    "train_feat = torch.cat(train_feat, axis=0)\n",
    "test_feat = torch.cat(test_feat, axis=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_feat = torch.from_numpy(sc.fit_transform(train_feat))\n",
    "test_feat = torch.from_numpy(sc.transform(test_feat))\n",
    "\n",
    "for i, data in enumerate(train_dataset):\n",
    "    data.x_feat = train_feat[i].unsqueeze(0)\n",
    "\n",
    "for i, data in enumerate(test_dataset):\n",
    "    data.x_feat = test_feat[i].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167bf06b-c717-477f-bfcb-251a0fc2bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "data_loader = DataLoader(datasets, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4fcce-2a3b-4868-94cf-9d941c63fded",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde77d00-5597-4195-9fa5-96acd3649773",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7735e0-f581-4bd5-97a8-ca21042e8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, in_attr, dim, out_channels):\n",
    "        super(GINE, self).__init__()\n",
    "\n",
    "        self.attr1 = Sequential(Linear(in_attr, in_channels), BatchNorm1d(in_channels), ReLU())\n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(in_channels, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr2 = Sequential(Linear(in_channels, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr3 = Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr4 = Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr5 = Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.lin1 = Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.lin2 = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        edge_attr = self.attr1(edge_attr)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr2(edge_attr)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr3(edge_attr)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr4(edge_attr)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr5(edge_attr)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "        \n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b59b767-83ce-4ca5-b20a-2fd757af246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(in_channels, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv4 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv5 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "        self.lin2 = Linear(dim, out_channels)\n",
    "        self.lin3 = Linear(out_channels, out_channels)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fafd152-2a49-4276-85de-06958a565f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, dim)\n",
    "        self.conv2 = GCNConv(dim, dim)\n",
    "        self.conv3 = GCNConv(dim, dim)\n",
    "        self.conv4 = GCNConv(dim, dim)\n",
    "        self.lin = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789f7ab1-9725-4307-86d6-c64800f952e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, dim)\n",
    "        self.conv2 = SAGEConv(dim, dim)\n",
    "        self.conv3 = SAGEConv(dim, dim)\n",
    "        self.conv4 = SAGEConv(dim, dim)\n",
    "        self.lin = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "076a8ea7-489f-4c71-a2fa-dff23f7b485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEHP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(SAGEHP, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, dim)\n",
    "        self.pool1 = SAGPooling(dim, ratio=0.8)\n",
    "        self.conv2 = SAGEConv(dim, dim)\n",
    "        self.pool2 = SAGPooling(dim, ratio=0.8)\n",
    "        self.conv3 = SAGEConv(dim, dim)\n",
    "        self.pool3 = SAGPooling(dim, ratio=0.8)\n",
    "        self.conv4 = SAGEConv(dim, dim)\n",
    "        self.pool4 = SAGPooling(dim, ratio=0.8)\n",
    "        self.lin = Linear(dim*2, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "        \n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "        \n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "        \n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x, edge_index, _, batch, _, _ = self.pool4(x, edge_index, None, batch)\n",
    "        x4 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "        \n",
    "        x = x2 + x3 + x4\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ffba7f3-45e4-495d-a44d-8c9bd2fb90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(Graph, self).__init__()\n",
    "        self.conv1 = GraphConv(in_channels, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.conv3 = GraphConv(dim, dim)\n",
    "        self.conv4 = GraphConv(dim, dim)\n",
    "        self.lin = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377f1cd-b88e-4662-831b-4480b173f1b4",
   "metadata": {},
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a9fc53d-c55d-41a4-8083-e1aabf5fb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lin1 = Linear(in_channels, dim)\n",
    "        self.lin2 = Linear(dim, dim)\n",
    "        self.lin3 = Linear(dim, dim)\n",
    "        self.lin4 = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x).relu()\n",
    "#         x = self.lin3(x).relu()\n",
    "        x = self.lin4(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a19426-45fa-4120-bc5e-e46b44a7f5c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb2dbed4-82bb-45f7-9cc1-ef577c1b1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, layer_num=2):\n",
    "        super(CrossNet, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.kernels = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "\n",
    "        for i in range(self.kernels.shape[0]):\n",
    "            nn.init.xavier_normal_(self.kernels[i])\n",
    "        for i in range(self.bias.shape[0]):\n",
    "            nn.init.zeros_(self.bias[i])\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_0 = inputs.unsqueeze(2)\n",
    "        x_l = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            xl_w = torch.tensordot(x_l, self.kernels[i], dims=([1], [0]))\n",
    "            dot_ = torch.matmul(x_0, xl_w)\n",
    "            x_l = dot_ + self.bias[i] + x_l\n",
    "        x_l = torch.squeeze(x_l, dim=2)\n",
    "        return x_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82eb29-3e47-45f7-afd4-3c28c6963fd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multi Input Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f8e2a6a-ba42-4c4a-8de0-bf276fd3976b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, in_xs, in_attr, in_xfeats, dim, out_channels):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.gine = GINE(in_xs, in_attr, dim, 128)\n",
    "#         self.mlp_num = MLP(in_xfeats, dim, 128)\n",
    "#         self.bn = BatchNorm1d(256)\n",
    "#         self.lin = Sequential(Linear(256, 128), BatchNorm1d(128))\n",
    "#         self.lin2 = Sequential(Linear(128, 128), BatchNorm1d(128))\n",
    "#         # Deep & Cross Network\n",
    "#         self.crossnet = CrossNet(128)\n",
    "#         self.mlp_cross = MLP(128, 256, 128)\n",
    "        \n",
    "#         self.bn_cat = BatchNorm1d(256)\n",
    "#         self.mlp_cat = MLP(256, 256, 128)\n",
    "#         self.dropout = Dropout(p=0.5)\n",
    "#         self.out = Linear(128, out_channels) # 256\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_attr, batch, x_feat):\n",
    "#         x = self.gine(x, edge_index, edge_attr, batch)\n",
    "#         x_feat = self.mlp_num(x_feat)\n",
    "#         concat = torch.cat((x, x_feat),dim=1)\n",
    "#         x = self.bn(concat)\n",
    "#         x = self.lin(x)\n",
    "#         x = self.lin2(x)\n",
    "        \n",
    "#         # Deep & Cross Network\n",
    "#         hl = self.mlp_cross(x)\n",
    "#         xl = self.crossnet(x)\n",
    "#         x = torch.cat((xl, hl), dim=1)\n",
    "#         x = self.bn_cat(x)\n",
    "#         x = self.mlp_cat(x)\n",
    "#         x = self.dropout(x)\n",
    "#         out = self.out(x)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d00365a0-505e-4adf-b703-f17774e8e674",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, in_xs, in_attr, in_xfeats, dim, out_channels):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.gin = GCN(in_xs, dim, 128)\n",
    "#         self.mlp_num = MLP(in_xfeats, dim, 128)\n",
    "#         self.bn = BatchNorm1d(256)\n",
    "#         # Deep & Cross Network\n",
    "#         self.crossnet = CrossNet(256, layer_num=2)\n",
    "#         self.mlp_cross = MLP(256, 256, 256)\n",
    "        \n",
    "#         self.bn_cat = BatchNorm1d(256) #64+in_xfeats\n",
    "#         self.dropout = Dropout(p=0.5)\n",
    "#         self.out = Linear(256, out_channels) # 256\n",
    "\n",
    "#     def forward(self, x, edge_index, batch, x_feat):\n",
    "#         x = self.gin(x, edge_index, batch)\n",
    "#         x_feat = self.mlp_num(x_feat)\n",
    "#         concat = torch.cat((x, x_feat),dim=1)\n",
    "#         x = self.bn(concat)\n",
    "        \n",
    "#         # Deep & Cross Network\n",
    "#         hl = self.mlp_cross(x)\n",
    "# #         xl = self.crossnet(x)\n",
    "# #         x = torch.cat((xl, hl), dim=1)\n",
    "#         x = self.bn_cat(hl)\n",
    "#         x = self.dropout(x)\n",
    "#         out = self.out(x)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a339c47-6c9b-408d-82ee-77e1efefdebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class DCN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(DCN, self).__init__()\n",
    "#         # Deep & Cross Network\n",
    "#         self.crossnet = CrossNet(in_channels)\n",
    "#         self.mlp_cross = MLP(in_channels, 256, 128)\n",
    "        \n",
    "#         self.bn = BatchNorm1d(128+in_channels)\n",
    "#         self.mlp_cat = MLP(128+in_channels, 512, 256)\n",
    "#         self.bn_out = BatchNorm1d(256)\n",
    "#         self.dropout = Dropout(p=0.5)\n",
    "#         self.out = Linear(256, out_channels) # 256\n",
    "\n",
    "#     def forward(self, x):        \n",
    "#         # Deep & Cross Network\n",
    "#         hl = self.mlp_cross(x)\n",
    "#         xl = self.crossnet(x)\n",
    "#         x = torch.cat((xl, hl), dim=1)\n",
    "#         x = self.bn(x)\n",
    "#         x = self.mlp_cat(x)\n",
    "#         x = self.bn_out(x)\n",
    "# #         x = self.dropout(x)\n",
    "#         out = self.out(x)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14c8bc7f-07c8-43d0-94c8-3b60f56ce02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_xs, in_xfeats, dim, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.g = SAGEHP(in_xs, dim, 128) # SAGE\n",
    "        self.mlp = MLP(in_xfeats, dim, 128)\n",
    "        self.lin = Linear(256, 128)\n",
    "        self.lin2 = Linear(128, 128)\n",
    "        self.cross = CrossNet(128, layer_num=2)\n",
    "        self.mlp_cross = MLP(128, 128, 128)\n",
    "        self.mlp_out = MLP(256, 256, 256)\n",
    "        self.out = Linear(256, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, x_feat):\n",
    "        x = self.g(x, edge_index, batch)\n",
    "        x_feat = self.mlp(x_feat)\n",
    "        concat = torch.cat((x, x_feat), dim=1)\n",
    "        x = self.lin(concat)\n",
    "        x = F.dropout(x, p=0.3, training=self.training) # SAGE: 0.3\n",
    "        x = self.lin2(x).relu()\n",
    "        \n",
    "        cross = self.cross(x)\n",
    "        mlp_cross = self.mlp_cross(x)\n",
    "        concat2 = torch.cat((cross, mlp_cross), dim=1)\n",
    "        \n",
    "        x = self.mlp_out(concat2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training) # SAGE: 0.3\n",
    "        out = self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436d70d-f407-4829-bc1a-70ff24aef54d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "711f76cb-8705-4ebf-a0ce-2b21ac50f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "    c=0\n",
    "    correct=0\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, data.x_feat.float())  # Perform a single forward pass. , data.edge_attr.float()\n",
    "    \n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        c=c+1\n",
    "        correct+=loss.cpu().detach().numpy()\n",
    "    return correct/c\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    c=0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, data.x_feat.float()) # , data.edge_attr.float()\n",
    "    \n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        correct += loss.cpu().detach().numpy()  # Check against ground-truth labels.\n",
    "        c=c+1\n",
    "    return correct / c  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08571f39-3903-4f7a-8fa8-769e373c125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features = 9\n",
    "hidden_channels = 64 # SAGE: 32\n",
    "num_feats = 21\n",
    "num_classes = 1\n",
    "num_attr = 3\n",
    "\n",
    "model = Net(num_node_features, num_feats, hidden_channels, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #0.001\n",
    "criterion = torch.nn.L1Loss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.85, patience=3, min_lr=0.000001) #factor=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbf307aa-531c-49ac-87f3-b48da24513e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "Epoch: 001, Train MAE: 38.2950, Test MAE: 24.8181\n",
      "Epoch: 002, Train MAE: 26.2166, Test MAE: 25.5509\n",
      "Epoch: 003, Train MAE: 24.4778, Test MAE: 22.2345\n",
      "Epoch: 004, Train MAE: 23.8731, Test MAE: 22.6548\n",
      "Epoch: 005, Train MAE: 23.3993, Test MAE: 21.3244\n",
      "Epoch: 006, Train MAE: 23.3232, Test MAE: 21.7249\n",
      "Epoch: 007, Train MAE: 22.6347, Test MAE: 21.0786\n",
      "Epoch: 008, Train MAE: 22.6281, Test MAE: 21.3390\n",
      "Epoch: 009, Train MAE: 22.0837, Test MAE: 24.0697\n",
      "Epoch: 010, Train MAE: 22.0191, Test MAE: 22.5267\n",
      "Epoch: 011, Train MAE: 21.9446, Test MAE: 20.4270\n",
      "Epoch: 012, Train MAE: 21.7741, Test MAE: 20.5988\n",
      "Epoch: 013, Train MAE: 21.6595, Test MAE: 20.8628\n",
      "Epoch: 014, Train MAE: 21.4537, Test MAE: 20.9217\n",
      "Epoch: 015, Train MAE: 21.4372, Test MAE: 20.5126\n",
      "Epoch: 016, Train MAE: 21.2446, Test MAE: 20.3886\n",
      "Epoch: 017, Train MAE: 20.9087, Test MAE: 19.8668\n",
      "Epoch: 018, Train MAE: 21.0030, Test MAE: 19.7306\n",
      "Epoch: 019, Train MAE: 20.8438, Test MAE: 20.2575\n",
      "Epoch: 020, Train MAE: 20.8223, Test MAE: 20.3925\n",
      "Epoch: 021, Train MAE: 20.6841, Test MAE: 20.5533\n",
      "Epoch: 022, Train MAE: 20.8997, Test MAE: 20.1827\n",
      "Epoch: 023, Train MAE: 20.4353, Test MAE: 20.0330\n",
      "Epoch: 024, Train MAE: 20.4078, Test MAE: 19.6098\n",
      "Epoch: 025, Train MAE: 20.3698, Test MAE: 19.8246\n",
      "Epoch: 026, Train MAE: 20.3001, Test MAE: 19.7003\n",
      "Epoch: 027, Train MAE: 20.2215, Test MAE: 20.5080\n",
      "Epoch: 028, Train MAE: 20.1953, Test MAE: 19.8282\n",
      "Epoch: 029, Train MAE: 19.9056, Test MAE: 20.1780\n",
      "Epoch: 030, Train MAE: 19.9360, Test MAE: 19.7986\n",
      "Epoch: 031, Train MAE: 19.8735, Test MAE: 19.4004\n",
      "Epoch: 032, Train MAE: 19.8782, Test MAE: 19.6522\n",
      "Epoch: 033, Train MAE: 19.9583, Test MAE: 20.2399\n",
      "Epoch: 034, Train MAE: 19.8192, Test MAE: 19.9082\n",
      "Epoch: 035, Train MAE: 19.8052, Test MAE: 20.7636\n",
      "Epoch: 036, Train MAE: 19.6299, Test MAE: 19.2450\n",
      "Epoch: 037, Train MAE: 19.5726, Test MAE: 19.2139\n",
      "Epoch: 038, Train MAE: 19.5018, Test MAE: 19.3599\n",
      "Epoch: 039, Train MAE: 19.3636, Test MAE: 19.6939\n",
      "Epoch: 040, Train MAE: 19.3853, Test MAE: 19.1522\n",
      "Epoch: 041, Train MAE: 19.4904, Test MAE: 19.1224\n",
      "Epoch: 042, Train MAE: 30.8250, Test MAE: 19.9464\n",
      "Epoch: 043, Train MAE: 19.8074, Test MAE: 19.0844\n",
      "Epoch: 044, Train MAE: 19.4426, Test MAE: 19.2137\n",
      "Epoch: 045, Train MAE: 19.3860, Test MAE: 19.4327\n",
      "Epoch: 046, Train MAE: 19.2986, Test MAE: 19.5009\n",
      "Epoch: 047, Train MAE: 19.2503, Test MAE: 19.4644\n",
      "Epoch: 048, Train MAE: 19.1225, Test MAE: 19.4836\n",
      "Epoch: 049, Train MAE: 19.0518, Test MAE: 19.2334\n",
      "Epoch: 050, Train MAE: 19.1810, Test MAE: 19.1429\n",
      "Epoch: 051, Train MAE: 19.1439, Test MAE: 19.1115\n",
      "Epoch: 052, Train MAE: 18.8626, Test MAE: 18.9917\n",
      "Epoch: 053, Train MAE: 18.8599, Test MAE: 19.1473\n",
      "Epoch: 054, Train MAE: 18.7885, Test MAE: 19.4801\n",
      "Epoch: 055, Train MAE: 18.7830, Test MAE: 19.0516\n",
      "Epoch: 056, Train MAE: 18.8407, Test MAE: 18.9508\n",
      "Epoch: 057, Train MAE: 18.8064, Test MAE: 19.0985\n",
      "Epoch: 058, Train MAE: 18.9009, Test MAE: 19.0246\n",
      "Epoch: 059, Train MAE: 18.7903, Test MAE: 18.9056\n",
      "Epoch: 060, Train MAE: 18.7801, Test MAE: 19.1956\n",
      "Epoch: 061, Train MAE: 18.7918, Test MAE: 19.0400\n",
      "Epoch: 062, Train MAE: 18.7301, Test MAE: 18.9347\n",
      "Epoch: 063, Train MAE: 18.7823, Test MAE: 19.0734\n",
      "Epoch: 064, Train MAE: 18.4948, Test MAE: 18.9608\n",
      "Epoch: 065, Train MAE: 18.5425, Test MAE: 18.9876\n",
      "Epoch: 066, Train MAE: 18.5058, Test MAE: 19.0342\n",
      "Epoch: 067, Train MAE: 18.6198, Test MAE: 19.0419\n",
      "Epoch: 068, Train MAE: 18.3837, Test MAE: 18.8934\n",
      "Epoch: 069, Train MAE: 18.3213, Test MAE: 18.8611\n",
      "Epoch: 070, Train MAE: 18.4262, Test MAE: 19.0557\n",
      "Epoch: 071, Train MAE: 18.4906, Test MAE: 19.1950\n",
      "Epoch: 072, Train MAE: 18.3604, Test MAE: 18.8882\n",
      "Epoch: 073, Train MAE: 18.3232, Test MAE: 18.9156\n",
      "Epoch: 074, Train MAE: 18.2228, Test MAE: 18.8313\n",
      "Epoch: 075, Train MAE: 18.2054, Test MAE: 18.8475\n",
      "Epoch: 076, Train MAE: 18.1782, Test MAE: 18.9156\n",
      "Epoch: 077, Train MAE: 18.1403, Test MAE: 18.7325\n",
      "Epoch: 078, Train MAE: 18.1648, Test MAE: 18.9356\n",
      "Epoch: 079, Train MAE: 18.0986, Test MAE: 18.8507\n",
      "Epoch: 080, Train MAE: 18.1390, Test MAE: 18.9090\n",
      "Epoch: 081, Train MAE: 18.1208, Test MAE: 18.9105\n",
      "Epoch: 082, Train MAE: 17.9239, Test MAE: 18.8882\n",
      "Epoch: 083, Train MAE: 17.9850, Test MAE: 19.1927\n",
      "Epoch: 084, Train MAE: 17.9352, Test MAE: 18.8556\n",
      "Epoch: 085, Train MAE: 17.9943, Test MAE: 18.9137\n",
      "Epoch: 086, Train MAE: 17.8457, Test MAE: 18.8912\n",
      "Epoch: 087, Train MAE: 17.8619, Test MAE: 18.7437\n",
      "Epoch: 088, Train MAE: 17.8030, Test MAE: 18.7994\n",
      "Epoch: 089, Train MAE: 17.7894, Test MAE: 18.8489\n",
      "Epoch: 090, Train MAE: 17.7359, Test MAE: 19.0895\n",
      "Epoch: 091, Train MAE: 17.6855, Test MAE: 18.9701\n",
      "Epoch: 092, Train MAE: 17.7073, Test MAE: 18.9107\n",
      "Epoch: 093, Train MAE: 17.6525, Test MAE: 18.8647\n",
      "Epoch: 094, Train MAE: 17.6600, Test MAE: 18.8788\n",
      "Epoch: 095, Train MAE: 17.5521, Test MAE: 18.8354\n",
      "Epoch: 096, Train MAE: 17.5421, Test MAE: 18.8673\n",
      "Epoch: 097, Train MAE: 17.5775, Test MAE: 18.8333\n",
      "Epoch: 098, Train MAE: 17.5507, Test MAE: 18.8077\n",
      "Epoch: 099, Train MAE: 17.4820, Test MAE: 18.9083\n",
      "Epoch: 100, Train MAE: 17.5257, Test MAE: 18.9534\n",
      "Epoch: 101, Train MAE: 17.4777, Test MAE: 18.9028\n",
      "Epoch: 102, Train MAE: 17.4572, Test MAE: 18.8527\n",
      "Epoch: 103, Train MAE: 17.3849, Test MAE: 18.8502\n",
      "Epoch: 104, Train MAE: 17.3790, Test MAE: 19.0875\n",
      "Epoch: 105, Train MAE: 17.4185, Test MAE: 18.8095\n",
      "Epoch: 106, Train MAE: 17.3599, Test MAE: 18.8032\n",
      "Epoch: 107, Train MAE: 17.2944, Test MAE: 18.7965\n",
      "Epoch: 108, Train MAE: 17.3387, Test MAE: 18.8698\n",
      "Epoch: 109, Train MAE: 17.3280, Test MAE: 18.8698\n",
      "Epoch: 110, Train MAE: 17.3126, Test MAE: 18.8880\n",
      "Epoch: 111, Train MAE: 17.2890, Test MAE: 18.8680\n",
      "Epoch: 112, Train MAE: 17.3030, Test MAE: 18.8922\n",
      "Epoch: 113, Train MAE: 17.2597, Test MAE: 18.9011\n",
      "Epoch: 114, Train MAE: 17.2105, Test MAE: 18.7901\n",
      "Epoch: 115, Train MAE: 17.2211, Test MAE: 18.9774\n",
      "Epoch: 116, Train MAE: 17.2259, Test MAE: 18.9087\n",
      "Epoch: 117, Train MAE: 17.2118, Test MAE: 18.9132\n",
      "Epoch: 118, Train MAE: 17.1751, Test MAE: 18.8693\n",
      "Epoch: 119, Train MAE: 17.1778, Test MAE: 18.8776\n",
      "Epoch: 120, Train MAE: 17.1902, Test MAE: 18.8904\n",
      "Epoch: 121, Train MAE: 17.2195, Test MAE: 18.8581\n",
      "Epoch: 122, Train MAE: 17.2083, Test MAE: 18.8609\n",
      "Epoch: 123, Train MAE: 17.2335, Test MAE: 18.8101\n",
      "Epoch: 124, Train MAE: 17.1581, Test MAE: 18.8379\n",
      "Epoch: 125, Train MAE: 17.1500, Test MAE: 18.8764\n",
      "Epoch: 126, Train MAE: 17.1140, Test MAE: 18.8297\n",
      "Epoch: 127, Train MAE: 17.0851, Test MAE: 18.9289\n",
      "Epoch: 128, Train MAE: 17.1577, Test MAE: 18.8211\n",
      "Epoch: 129, Train MAE: 17.0439, Test MAE: 18.8575\n",
      "Epoch: 130, Train MAE: 17.1428, Test MAE: 18.8657\n",
      "Epoch: 131, Train MAE: 17.1107, Test MAE: 18.8885\n",
      "Epoch: 132, Train MAE: 17.1019, Test MAE: 18.9146\n",
      "Epoch: 133, Train MAE: 17.1342, Test MAE: 18.8739\n",
      "Epoch: 134, Train MAE: 17.0649, Test MAE: 18.8360\n",
      "Epoch: 135, Train MAE: 17.0582, Test MAE: 18.8689\n",
      "Epoch: 136, Train MAE: 17.0835, Test MAE: 18.8753\n",
      "Epoch: 137, Train MAE: 17.0651, Test MAE: 18.8698\n",
      "Epoch: 138, Train MAE: 17.0626, Test MAE: 18.8564\n",
      "Epoch: 139, Train MAE: 17.0019, Test MAE: 18.8589\n",
      "Epoch: 140, Train MAE: 17.0123, Test MAE: 18.8527\n",
      "Epoch: 141, Train MAE: 17.0916, Test MAE: 18.8784\n",
      "Epoch: 142, Train MAE: 17.0498, Test MAE: 18.8854\n",
      "Epoch: 143, Train MAE: 17.0144, Test MAE: 18.8890\n",
      "Epoch: 144, Train MAE: 17.0188, Test MAE: 18.8965\n",
      "Epoch: 145, Train MAE: 17.0629, Test MAE: 18.9045\n",
      "Epoch: 146, Train MAE: 17.0427, Test MAE: 18.9148\n",
      "Epoch: 147, Train MAE: 17.0832, Test MAE: 18.8798\n",
      "Epoch: 148, Train MAE: 17.0423, Test MAE: 18.8952\n",
      "Epoch: 149, Train MAE: 17.0740, Test MAE: 18.8919\n",
      "Epoch: 150, Train MAE: 16.9675, Test MAE: 18.9005\n",
      "Epoch: 151, Train MAE: 16.9909, Test MAE: 18.8759\n",
      "Epoch: 152, Train MAE: 16.9842, Test MAE: 18.8518\n",
      "Epoch: 153, Train MAE: 16.9784, Test MAE: 18.8569\n",
      "Epoch: 154, Train MAE: 16.9375, Test MAE: 18.8881\n",
      "Epoch: 155, Train MAE: 16.9885, Test MAE: 18.8915\n",
      "Epoch: 156, Train MAE: 16.9416, Test MAE: 18.8791\n",
      "Epoch: 157, Train MAE: 17.0099, Test MAE: 18.8769\n",
      "Epoch: 158, Train MAE: 16.9375, Test MAE: 18.8904\n",
      "Epoch: 159, Train MAE: 17.0004, Test MAE: 18.9173\n",
      "Epoch: 160, Train MAE: 16.9691, Test MAE: 18.9023\n",
      "Epoch: 161, Train MAE: 16.9637, Test MAE: 18.9093\n",
      "Epoch: 162, Train MAE: 16.9686, Test MAE: 18.9042\n",
      "Epoch: 163, Train MAE: 17.0012, Test MAE: 18.9163\n",
      "Epoch: 164, Train MAE: 17.0581, Test MAE: 18.8810\n",
      "Epoch: 165, Train MAE: 16.9599, Test MAE: 18.9145\n",
      "Epoch: 166, Train MAE: 16.9986, Test MAE: 18.9079\n",
      "Epoch: 167, Train MAE: 16.9648, Test MAE: 18.8735\n",
      "Epoch: 168, Train MAE: 16.9873, Test MAE: 18.8821\n",
      "Epoch: 169, Train MAE: 16.9817, Test MAE: 18.8725\n",
      "Epoch: 170, Train MAE: 16.9002, Test MAE: 18.8764\n",
      "Epoch: 171, Train MAE: 16.9069, Test MAE: 18.8899\n",
      "Epoch: 172, Train MAE: 16.9018, Test MAE: 18.9060\n",
      "Epoch: 173, Train MAE: 16.9246, Test MAE: 18.8966\n",
      "Epoch: 174, Train MAE: 16.9369, Test MAE: 18.8736\n",
      "Epoch: 175, Train MAE: 16.9353, Test MAE: 18.8965\n",
      "Epoch: 176, Train MAE: 16.8622, Test MAE: 18.8811\n",
      "Epoch: 177, Train MAE: 17.0671, Test MAE: 18.9070\n",
      "Epoch: 178, Train MAE: 16.8866, Test MAE: 18.9083\n",
      "Epoch: 179, Train MAE: 16.9007, Test MAE: 18.8903\n",
      "Epoch: 180, Train MAE: 16.9227, Test MAE: 18.9014\n",
      "Epoch: 181, Train MAE: 16.8784, Test MAE: 18.8983\n",
      "Epoch: 182, Train MAE: 16.9173, Test MAE: 18.8995\n",
      "Epoch: 183, Train MAE: 16.9911, Test MAE: 18.9007\n",
      "Epoch: 184, Train MAE: 16.8668, Test MAE: 18.9162\n",
      "Epoch: 185, Train MAE: 16.9089, Test MAE: 18.9134\n",
      "Epoch: 186, Train MAE: 16.8897, Test MAE: 18.8896\n",
      "Epoch: 187, Train MAE: 16.9251, Test MAE: 18.9040\n",
      "Epoch: 188, Train MAE: 16.9234, Test MAE: 18.8942\n",
      "Epoch: 189, Train MAE: 16.8858, Test MAE: 18.8992\n",
      "Epoch: 190, Train MAE: 16.9673, Test MAE: 18.8895\n",
      "Epoch: 191, Train MAE: 16.9204, Test MAE: 18.8783\n",
      "Epoch: 192, Train MAE: 16.9932, Test MAE: 18.8839\n",
      "Epoch: 193, Train MAE: 16.9502, Test MAE: 18.8905\n",
      "Epoch: 194, Train MAE: 16.9043, Test MAE: 18.8894\n",
      "Epoch: 195, Train MAE: 16.9185, Test MAE: 18.8863\n",
      "Epoch: 196, Train MAE: 16.9361, Test MAE: 18.8867\n",
      "Epoch: 197, Train MAE: 16.8651, Test MAE: 18.8926\n",
      "Epoch: 198, Train MAE: 16.9263, Test MAE: 18.8807\n",
      "Epoch: 199, Train MAE: 16.9747, Test MAE: 18.8777\n",
      "Epoch: 200, Train MAE: 16.9761, Test MAE: 18.8791\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 200\n",
    "print('start train')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_acc = train(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    train_loss.append(train_acc)\n",
    "    test_loss.append(test_acc)\n",
    "    scheduler.step(test_acc)\n",
    "    print(f'Epoch: {epoch+1:03d}, Train MAE: {train_acc:.4f}, Test MAE: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35131580-625c-463b-ae2d-9d78aec742df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzTElEQVR4nO3dd3xc1Z338c9vZqQZq9qSZVuussC927IxGNtUU0wNLQRCW2DJwwZ4SHjChg0LG0ggwJIlIUsggYUAAQJkTQ3VxgZcMEZuuFdZlm3Jspoljaac549z1YvlonKZ3/v10kuje+/MnLma+c65555zrhhjUEop5T6eri6AUkqpI6MBrpRSLqUBrpRSLqUBrpRSLqUBrpRSLqUBrpRSLqUBrr7TRGS7iJzR1eVQqiNogCullEtpgCullEtpgKuYICJ+EfmtiOx2fn4rIn5nXW8ReUdESkSkWEQWiYjHWfczEckXkXIR2SAip3ftK1Gqnq+rC6BUJ7kHmA5MBAwwD/g34BfAT4BdQIaz7XTAiMgI4F+AqcaY3SKSBXg7t9hKtU5r4CpWXAX8hzFmnzGmELgf+KGzLgRkAkOMMSFjzCJjJwmKAH5gtIjEGWO2G2O2dEnplWqBBriKFf2BHQ3+3uEsA3gE2Ax8KCJbReRuAGPMZuAO4D5gn4i8IiL9Uaqb0ABXsWI3MKTB34OdZRhjyo0xPzHGZAMXAHfWtnUbY142xpzs3NcAD3dusZVqnQa4ihV/Bf5NRDJEpDdwL/AigIicJyLHi4gApdimk6iIjBCR05yTndVAFRDtovIr1YwGuIoVDwDLgVXAamCFswxgGPAxUAEsBv5gjJmPbf9+CCgC9gB9gH/t3GIr1TrRCzoopZQ7aQ1cKaVcSgNcKaVcSgNcKaVcSgNcKaVcqlOH0vfu3dtkZWV15lMqpZTrff3110XGmIymyzs1wLOysli+fHlnPqVSSrmeiOxoabk2oSillEtpgCullEtpgCullEvpfOBKqWMiFAqxa9cuqquru7oorhUIBBg4cCBxcXHt2l4DXCl1TOzatYvk5GSysrKw84Kpw2GMYf/+/ezatYuhQ4e26z7ahKKUOiaqq6tJT0/X8D5CIkJ6evphHcFogCuljhkN76NzuPvPFQH+ybq9/PcCvZKVUko15IoAX7ChkGcWbe3qYiilurGSkhL+8Ic/HNF9zz33XEpKStq9/X333cejjz56RM91LLkiwL0eIRzRC6EopVrXVoCHw+E27/vee+/Rs2fPDihVx3JNgEf1uhNKqTbcfffdbNmyhYkTJ3LXXXexYMECZs6cyQUXXMDo0aMBuOiii5gyZQpjxozh6aefrrtvVlYWRUVFbN++nVGjRnHTTTcxZswY5syZQ1VVVZvPm5uby/Tp0xk/fjwXX3wxBw4cAOCJJ55g9OjRjB8/nu9///sAfPbZZ0ycOJGJEycyadIkysvLj+o1u6IbodcjhKNaA1fKLe5/ey3f7i47po85un8K/37+mFbXP/TQQ6xZs4bc3FwAFixYwIoVK1izZk1dt7xnn32WtLQ0qqqqmDp1Kpdccgnp6emNHmfTpk389a9/5ZlnnuHyyy/njTfe4Oqrr271ea+55hp+97vfMXv2bO69917uv/9+fvvb3/LQQw+xbds2/H5/XfPMo48+ypNPPsmMGTOoqKggEAgc1T5xTw1c81spdZimTZvWqE/1E088wYQJE5g+fTp5eXls2rSp2X2GDh3KxIkTAZgyZQrbt29v9fFLS0spKSlh9uzZAFx77bUsXLgQgPHjx3PVVVfx4osv4vPZuvKMGTO48847eeKJJygpKalbfqTcUQMXrYEr5SZt1ZQ7U2JiYt3tBQsW8PHHH7N48WISEhI45ZRTWuxz7ff76257vd5DNqG05t1332XhwoW8/fbbPPjgg6xevZq7776buXPn8t577zFjxgw++OADRo4ceUSPD26qgRs7UkkppVqSnJzcZptyaWkpvXr1IiEhgfXr17NkyZKjfs7U1FR69erFokWLAPjLX/7C7NmziUaj5OXlceqpp/Lwww9TWlpKRUUFW7ZsYdy4cfzsZz9j6tSprF+//qie3x01cI/t3B6JGnxeHSiglGouPT2dGTNmMHbsWM455xzmzp3baP3ZZ5/NU089xahRoxgxYgTTp08/Js/7/PPPc8stt1BZWUl2djbPPfcckUiEq6++mtLSUowx3HbbbfTs2ZNf/OIXzJ8/H4/Hw5gxYzjnnHOO6rmlM2u1OTk55kgu6PDk/M088sEGNjxwNn6ftwNKppQ6WuvWrWPUqFFdXQzXa2k/isjXxpicptu6pgkFbA1cKaWU5YoA92mAK6VUM64IcK2BK6VUcxrgSinlUhrgSinlUu4IcGeO3LAGuFJK1XFHgGsNXCnVAZKSkg5reXejAa6UUi7lrgDXofRKqVbcfffdPPnkk3V/1150oaKigtNPP53Jkyczbtw45s2b1+7HNMZw1113MXbsWMaNG8err74KQEFBAbNmzWLixImMHTuWRYsWEYlEuO666+q2ffzxx4/5a2zKdUPplVIu8P7dsGf1sX3MfuPgnIdaXX3FFVdwxx13cOuttwLw2muv8cEHHxAIBPj73/9OSkoKRUVFTJ8+nQsuuKBd15988803yc3NZeXKlRQVFTF16lRmzZrFyy+/zFlnncU999xDJBKhsrKS3Nxc8vPzWbNmDcBhXeHnSLkiwHUgj1LqUCZNmsS+ffvYvXs3hYWF9OrVi0GDBhEKhfj5z3/OwoUL8Xg85Ofns3fvXvr163fIx/z888+58sor8Xq99O3bl9mzZ/PVV18xdepUbrjhBkKhEBdddBETJ04kOzubrVu38uMf/5i5c+cyZ86cDn/Nrghwj2iAK+UqbdSUO9Jll13G66+/zp49e7jiiisAeOmllygsLOTrr78mLi6OrKysFqeRPRyzZs1i4cKFvPvuu1x33XXceeedXHPNNaxcuZIPPviAp556itdee41nn332WLysVrmiDbx2BkINcKVUW6644gpeeeUVXn/9dS677DLATiPbp08f4uLimD9/Pjt27Gj3482cOZNXX32VSCRCYWEhCxcuZNq0aezYsYO+ffty0003ceONN7JixQqKioqIRqNccsklPPDAA6xYsaKjXmYdV9XAtR+4UqotY8aMoby8nAEDBpCZmQnAVVddxfnnn8+4cePIyck5rAsoXHzxxSxevJgJEyYgIvzmN7+hX79+PP/88zzyyCPExcWRlJTECy+8QH5+Ptdffz1R5+Izv/71rzvkNTbkiulkP99UxNV/XsrfbjmRqVlpHVAypdTR0ulkj43v3HSyHqeU4YjWwJVSqpYrAtznJHhU+4ErpVQdVwS4t7YGrm3gSnVret3ao3O4+++QAS4iARFZJiIrRWStiNzvLP8fEdkmIrnOz8QjK/KheWtr4BrgSnVbgUCA/fv3a4gfIWMM+/fvJxAItPs+7emFEgROM8ZUiEgc8LmIvO+su8sY8/oRlPWw6GyESnV/AwcOZNeuXRQWFnZ1UVwrEAgwcODAdm9/yAA39uu0wvkzzvnp1CTVofRKdX9xcXEMHTq0q4sRU9rVBi4iXhHJBfYBHxljljqrHhSRVSLyuIj4O6qQGuBKKdVcuwLcGBMxxkwEBgLTRGQs8K/ASGAqkAb8rKX7isjNIrJcRJYf6aGVzkaolFLNHVYvFGNMCTAfONsYU2CsIPAcMK2V+zxtjMkxxuRkZGQcUSHra+DRI7q/Ukp9F7WnF0qGiPR0bvcAzgTWi0ims0yAi4A1HVXI+tkIO+oZlFLKfdrTCyUTeF5EvNjAf80Y846IfCoiGYAAucAtHVVIj9bAlVKqmfb0QlkFTGph+WkdUqIWaA1cKaWac8lITK2BK6VUU+4IcL2gg1JKNeOOAPfqSEyllGrKHQHu1MB1NkKllKrnjgD3aA1cKaWaclWA62yESilVzx0BrrMRKqVUM64IcI9HENEauFJKNeSKAAc7mEdr4EopVc81Ae4R0dkIlVKqAdcEuM8jRPSq9EopVcc1Ae7xaA1cKaUack2A+zyiQ+mVUqoB1wS4V09iKqVUI64KcO1GqJRS9dwT4KI1cKWUasg9Ae7VGrhSSjXkngDXGrhSSjXingDXboRKKdWIuwJcB/IopVQdFwW4R2vgSinVgIsCXK+JqZRSDbkowD0a4Eop1YBrAlyH0iulVGOuCXCvaIArpVRD7glwrYErpVQj7gpw7YWilFJ1XBXgOhJTKaXquSrAdS4UpZSq56oA1xq4UkrVc0+Ai9bAlVKqIfcEuFcIR6NdXQyllOo23BPgIsRiBfzdVQWs2lXS1cVQSnVDrglwnyc2a+C/em8d//Pl9q4uhlKqG3JNgHs8QgzmN6FIVAcwKaVa5JoAj9UaeCRqtPeNUqpFrglwj0eIxF5+EzFGL2ShlGqRawLczkYYewkeiRidQkAp1aJDBriIBERkmYisFJG1InK/s3yoiCwVkc0i8qqIxHdoQWN0NsJw1MTk61ZKHVp7auBB4DRjzARgInC2iEwHHgYeN8YcDxwA/qnDSknszgceMdoGrpRq2SED3FgVzp9xzo8BTgNed5Y/D1zUEQWsFauzEUaiJiabjpRSh9auNnAR8YpILrAP+AjYApQYY8LOJruAAa3c92YRWS4iywsLC4+4oLE4H7gxtvkkrCcxlVItaFeAG2MixpiJwEBgGjCyvU9gjHnaGJNjjMnJyMg4slISmwFe+3pj7XUrpdrnsHqhGGNKgPnAiUBPEfE5qwYC+ce2aI15PXYovYmhZpTaJiNtA1dKtaQ9vVAyRKSnc7sHcCawDhvklzqbXQvM66AyAnYuFIit2mjta43G0JeWUqr9fIfehEzgeRHxYgP/NWPMOyLyLfCKiDwAfAP8uQPLiddrAzwcNfi8HflM3UdtzVvbwJVSLTlkgBtjVgGTWli+Fdse3il8HhvgsVQbrR2BGUtHHUqp9nPNSEyP1NfAY0V9G7h2I1RKNeeaAK+tgcfSvCDaC0Up1RbXBLi3NsBjqAmlrg1cA1wp1QIXBbgtaizVRrUNXCnVFhcFuP0dS2FWe7QRS69ZKdV+LgrwGKyBOycvY+k1K6Xaz0UBbn/HUphpG7hSqi0uCnBb1FgKs7C2gSul2uCeAJfYG8gT1X7gSqk2uCfAnW6EsTSsPKz9wJVSbXBdgMdSDVwH8iil2uKaAK8diRmLbeBRA9EYet1KqfZxTYB7PLE3nWzDo41YGoGqlGof1wS4LwYDvOHRRiy9bqVU+7gmwD0xeUGH+t4nsdR0pJRqH9cEuM8bewHesMdNLM3CqJRqH9cEeF0NPIbaghu2gWtfcKVUU64J8Po28NgJskZt4DH0xaWUah/XBHjdfOCxk9+NmotiqelIKdU+Lgzw2Enwhm3gsTQCVSnVPi4M8C4uSCdq2GyiNXClVFOuC/BYOpnXMLS1G6FSqin3BHgMzkaoA3mUUm1xT4DH4GyEkUjDgTyxc+ShlGof1wR47UCeWKqBN/yu0vxWSjXlmgCvbUKJpbbgxkPpNcGVUo25J8Br5wOPoQDXNnClVFtcF+AxVQOPaC8UpVTrXBPgPuey9DF1ElP7gSul2uCaAE+M9xLv9bD/YE1XF6XTaD9wpVRbXBPgIkJGsp/C8mBXF6XTNG4D15OYSqnGXBPgAL2T/ewrr+7qYnSaxpNZdWFBlFLdkqsCPCMptmrgEa2BK6Xa4KoA75MSuwGubeBKqabcE+ChKm7ffCNjq74iFCPtCQ0H72gvFKVUU+4J8N3f0PfgesbJVopjpCdKoxp4DHWfVEq1j3sCPG8pAIlSzb6y2GhG0SvyKKXacsgAF5FBIjJfRL4VkbUicruz/D4RyReRXOfn3A4tad4yABKpprAiNnqihKOGOG/sjUBVSrWPrx3bhIGfGGNWiEgy8LWIfOSse9wY82jHFc9hTIMaeFVM1cDjvR5CkYhe1Fgp1cwha+DGmAJjzArndjmwDhjQ0QVrpHgrVO4HIInqmOmJEo4a/HFeoPHc4EopBYfZBi4iWcAkYKmz6F9EZJWIPCsivVq5z80islxElhcWFh5ZKZ3mEwI9SfUG2RcjAR6NGvw+Zw4YbUJRSjXR7gAXkSTgDeAOY0wZ8N/AccBEoAB4rKX7GWOeNsbkGGNyMjIyjqyUeUvBnwL9J5HqCcZUDTzeCXA9iamUaqpdAS4icdjwfskY8yaAMWavMSZijIkCzwDTOqyUM26DS58DfzLJnuqYGU4f0Rq4UqoN7emFIsCfgXXGmP9ssDyzwWYXA2uOffEcadkw7AzwJzu9UGKjBm4D3Ft3WymlGmpPL5QZwA+B1SKS6yz7OXCliEwEDLAd+OcOKF9j8UkETBV7y4KEIlHivO7pxn4kItqNUCnVhkMGuDHmc0BaWPXesS/OIfiT8EcrqQlHWJlXQk5WWqcXoTOFo1F8Hg9ej8TUpeSUUu3jripsfBIeE6GHhFi4qairS9PhIlGD1yN4PaI1cKVUM64LcIBp/eNZtKmNLonRCOzpuCb5zhKJGnxewecRnU5WKdWMuwLcbwN8VlYPVuaVUFoZanm7jf+Ap2ZA8bZOLNyxpzVwpVRb3BXgTg18+oB4oga+3NJKM0rZbvv7gLsDPBw1eKW2Bq4BrpRqzF0B7tTAR/QSUgI+Pvx2b8vbBcvs79ogd6n6GrhHa+BKqWbcFeBODdwXruTccZl8uHYPVTWR5ttVl9rfZQWdWLhjr1EbuM4HrpRqwpUBTrCcCyb052BNhE/Wt1ALrwvw/M4rWwewNXDbjVBnI1RKNeWuAHeaUKip4ITsdPok+3krt4VmkmqnCaXc3TVw2waODXBtQlFKNeGuAK+tgdccxOsRLpjQn4/X7eX2V75hb1mD+VG+YzVwn/ZCUUq1oD1D6buPuiaUCgBuP2MYIvD84h3EeT08etkEZ33tSUx318AjUYPP6Uao/cCVUk25K8B98eCNh5pyAJIDcdwzdzR5xVUs3ba/frvaGnhlEYSD4PN3QWGPXjhq8HqdfuB6ElMp1YS7mlDA1sKdGnitaUPTyCuuoqC0yi6oLrNBD65uB49Eo7YfuFfbwJVSzbkvwP1JUNM8wAGWbSu2C6pLofdwe9vFfcHDDfqBay8UpVRT7gvw+ORmNfBRmSkk+302wMM1EK6CjJF2pYsDPFrbBi46H7hSqjkXBniirYGX7ISQ7Xni9QhTsnqxdFsx1RUH7HZ9RtnfLg7w2hq4z+PRNnClVDPuC3B/kg3v30+DJU/WLT5haDqb91Vw9sNv2wWpA217uavbwOsns9IauFKqKXf1QgEbyrWTVBVuqFt81fTBJPq9rFyyD0rB+FOQ5ExX9wUPO00oPq8QDLcwZYBSKqa5sAaeXH+7ZGfdzZRAHNecmMUFIxMB2Fbhg6Q+cHB/00dwhdor8NQNpdcauFKqCfcFeO1gHn9qowCvNbmvfUlf7ApBQhpUujPAw3UBjo7EVEq1yH0BnjYUkvrB5B/aE5Thmkark6kE4JNt1ZCQ7toAj2gNXCl1CO4L8BNugTtWOb1MDJTtarzeGYX59d4o8/MimMr94MI+1GFn6LxPT2IqpVrhvgAXsUPjew62f5fkNV5fXYZBOHfK8SwuEMREuOGpj9hdUtX5ZT0KtVOf1A3k0QBXSjXhvgCvVRfgTdrBq0sRfwoPXzaJ28+fDsDePbv5wTNLGs9Y2M3V1sBtP3BtA1dKNefeAE8ZAOKxAb7oMdjxpV0eLINACgCJvfoC8Nh5AyksD3LRk1/wxeZWrqPZzdS3gWsTilKqZe4NcG+cDfGN78Mn/wFf/JddXl0KgVR7O8HOkTIyJcSr/3wiCfFervrTUubldv++4bU1bl9dDVynk1VKNebeAAfbjLJntb29/QuIhO1MhHUBnm5/V+5n7IBU3r1tJtOGpvGzN1axeMv+lq+n2U1oDVwpdSjuD3CA5Ew7R3jBSlsD99smFBJ7298HbbNJIM7LH66aTK+EeK58Zgnj7vuAvy3Pa+GBu17DAPdpgCulWuDyAB9if5/vNJ9seBdK8+qaTohLAF+gUV/w3kl+/vfWGTx22QRysnrxr2+u5vNN3a9dPNwgwD16ElMp1QJ3B3jO9fC9P8Hws6DPaPj8cVsDn3K9XS/iDOYpbnS3vikBLpkykKevyeG4jCTueDW32zWnROrawD1aA1dKtcjdAZ7cD8ZfZm9nzQQThWk3w6Cp9du0MZw+JRDHAxePpagiyEtLdwBgusmgn8Zt4B6tgSulmnHfbIStmXQ1HCyE03/ReHlLw+lLdtpLriX3Y2pWGicdl85Tn23lo2/3sqesmuevn0ZW78RGd4lGDR6PdPCLqKdt4EqpQ3F3DbyhzPFw2XONZysESOhtL24MUL4Xnj0HfjsOnjunbrjjbacPo6giyOZ9FZRVhbj8j4t56rMtLNlqg/+3H29k5m/mU14d6rSX09JQ+u5ydKCU6h6+OzXw1jSsgb9/F+R/DeMug9V/g63z4fjTmZ6dzhs/OolhfZPYU1rNzS8s56H31wMwd1wm7662F4X4x5o9XJYz6OjLtO5tO/Do7F+3uknTGnjtMp+3844ClFLd23enBt6ahHR7YnPd2/DtPJh9F1z4pF2+/Nm6zaYM6UVKII7hfZNZcNeprLpvDldOG8y7qwsYOyCFwWkJzMs9RpdnW/ECLHu6fsKTFkQaDOTxOqGtFzZWSjUUAwHudCn8+y2QMQpOut1OhjXpatjwPhzY3uLdUgJx/OqMdFZk/YGXTy7iokkD+GJLUcvzqez6utHVgQ6pYBVEw7bNvhW1Ae7xCF6RRsuUUgpiIsCd0Zj+ZLjyZfDF279zbrBB/qczYdui5vcryUP+Zy5pez4nZdEvuWhCP4yBH//1G56cv5mFGwuprAnbbV+/Dt75v+0rT/keqNhjb7dxubeGQ+m9ThOK9kRRSjX03Q/wQSfAiLlwzVuQll2/vFcW3PSpnfjqb9fZYfi1di6BZ061l2M76TYo3kJ28efcdtrx7C2r5pEPNnDNs8uY/qtPeOKtL22vll1fQagdsx0WrKq/3cYFl1tsA9cr0yulGjhkgIvIIBGZLyLfishaEbndWZ4mIh+JyCbnd6+OL+4RSB1ga94Zw5uv6zMKTvuF7aWSt8QuqyyGFy6yNfYbP4LT/x1SB8GXT3DnmcP57K5TWXnvHF64YRozh2WQu/RTe79IDS+++SbVu1bDvnXNn2vBQ/DChVCQW7+srPU29Ub9wL3236Q1cKVUQ+2pgYeBnxhjRgPTgVtFZDRwN/CJMWYY8Inzt/scfwZ4/fYkJ9h28XAVXPJnyBgBXh/MuB12LoZ5t0IkRGpCHLOGZ/DkVZN5/GSDQYgiFK/+gNAL34OXL4dog5GdxsA3L8LWBfbkZa+h4IlrVxNK014oSilV65ABbowpMMascG6XA+uAAcCFwPPOZs8DF3VQGTuWPwmOPx3WvWODdv07kDIQ+k+q32bqjXDKzyH3Jfj0gUZ3Tz2wBuk9HE/fMdwS9z7JNfugZCf7vp5X32+7aCOU5mEQe+Ky/yRIyWxXDTxQuYdApAJAp5RVSjVyWG3gIpIFTAKWAn2NMbWNuHuAvq3c52YRWS4iywsLW+910aVGnmevrbl1Pmz5FEbOtfOo1BKBU34Goy6wXQDDQQhW2Hbz/BUwYDIMmUG8CbLWDGW3SWPDW48x+5EF3PlaLn958c8APBa2w/5DfcbZuczbCPDasB781qVM+vY3QJu9DpVSMajdAS4iScAbwB3GmLKG64ytarZ4fG+MedoYk2OMycnIyDiqwnaYEefY3iovXQ7hahvgLZlyHVQV2/7jv8+xPwf32Rp19mwAak66ky2Dr2Cmdw3TkwtZsmU/oyqWUtxjKDtG3swvQ1dx4vv9WFAQR7hkV8vPA0SNoQ8HiCvbSfqBlQCUdeJIUKVU99euABeROGx4v2SMedNZvFdEMp31mcC+jiliJ0hIgxs/tr1UkjNhyIyWt8s+FVIHwz/utoODamzTBv0nw4hz4aZPmXTWNcz8/k/B6+c3g5bw5U+mk8M60iacy3/9IIec7/+Ca8+cytaaVCIl+ewoqqh//DVvwB9nQaiacMQwzrMVgKSKbaR4g7z+deuBr5SKPYccSi8iAvwZWGeM+c8Gq94CrgUecn7P65ASdpa0bLhlEdQctCcuW+LxwORrYP4DcP4TMHQWbF8EA3NsM8uAKXa7xN4w7lJY+YpdHgnC6AvxeIRzxmUCsLVqAv7lb3Pho++QPXgQ157QnwsW3ouU7oLti4hERzDOsw0AwXDjsCr++6s87jhjGD0T4u2VhyIhSEy3bfeiQ+yVijXtqYHPAH4InCYiuc7PudjgPlNENgFnOH+7m89fP3KzNTPvhFu+sNPYJve1Qd1SeE67GUIH4as/2VGfg09otDo723ZrvGdmKiWVIRa/+XukdBdGPFSseot3VhUwTrYR7WHLc/nAYqpCEX7zv0spf/Wf4dFh8Eg2PDYKfjUAnpvbuC97U8uegadm2tBXSn0nHLIGboz5HGitenf6sS2OC3i80G/sobfrPxGGnAzFW2HOg83XpwwA4LJhHi45KZvqP77DyqrjKIikMXHV2yyPnM3TCXl4hp8FG/9Bv8qNXDVtNqd8czt+z0oWpZ7L8SPG0i+4FTFROznXkidtl8emguUw/0GoOgAb/wGjzj+6faCU6ha++7MRdqUfvGL7g/fo2XxdSn/7e+GjeEp2kmCqSLroaWpWL6ff1v9g0YVBEt4tsidIywugYBUPjvoQvCv4ZOhP+fGWqVQuipCZOpX7zx/NnFAVfPogwZQs9mSezuD0RKT2yGDZ0za8/Smw4i+tB3gkDLuWweATj7xJprLY9m/vN+7I7l/LGMhbZpunPN6jeyylvqO++0Ppu5I/ueXwBnuyNOcGO9Vtj15wwz84btJsLrj0ehAPGZ/cYbfLnAj9xtsRnJ/+EsZewunX/Btf/Ow0Hr1sAulJ8dz84gp+7bmJisSB+N+4huAT03jrPy4md/kXULQJvvwdDJtj+7Nv/qi+++L6d2HnUhuWAAt+ZedJf/u2+oFIlcXNLknXqj1r7EnYP86Gwo1Hts9qrX8Xnp0Di39/6G2ry+wgqWM9W2P+CghVHdvHVOoYks68SEBOTo5Zvnx5pz2fa634i23qiNTA5X+xJ0pfuhRm/hRO/XmjGmkwHOFX767jpaU7MdEwNyd9yVWpK+m5P5d4QsT1SEY8XrjuPXu/302GobPtKNNlT9sHGXQCzLoLXrnKXqauZAccd7o9YfvunbZmfuZ9EJdopxzY/rkN9T6j4NJnIakP7FltL5bhT4ZgGRx3Klzx4pG9/kgY/jAd9m+CxAy4fRXEJ7S8rTHw0mX2i2nkeTD3MfsaDkfVAQj0bHzUsfEDO6J23OVwyTNH9jqiUdi+0PZSCqQc2WMoBYjI18aYnGbLNcBdoroUAqmtrt5XVs0XW4qYPbwPaYnxLF65jsrXf8SkhH1sP+t5RoyZRKLfB1/9CfPRvyM1FTD1RjabAWTm/heJ4RKIS4B/WQ4b34cPfwGhSkg/3l7VqHaumPgke/3RpAxY/boN7+n/B774LxumN34EuS/bNveR59n7nHCLPW+Qv8KOZh0yw57YfeUHdgKwKddB3lJ7vqA2RDd/bCcS+/IJGHup7R0UnwhZM+xFq2u3W/6snQlyxFzY9CGYiO0dNGyO7dsfroaBU+3UBQe2Q9pQ2+MoErJHJiv/CsVb7Hw3w8+2P4FUeOVKp6dP0H75ZTldS42xX2AVe+2ArvLddiqGhDQoybPjAhA48VZ7AnvJH+w+G5hj79t/IvQebh+7uhQ8Pug7xr62QAr0mwAYe+S0Z7X9MvL54eP7oedgmPNLe9RWthuqS+w+27/F/t1nNAyebpvdSvMaDxTzxkGPNPtY4rX7ryzflrnmoD3hXlPZ+Hao0u7DPqOhptzus7ge9n3iC9gy9xxse13VVELheijdZSsePr9dH5dgu9uWFdgRyR6v3b+halsG8dr9kNAL0ofZL+2qElsR6DPKNjXWVNrn9fjsYwXL7ZfuwUL7f01Ig55DbMWhcn/9hcxLd9lpmwMpdlmwwpatlog90qwsss/h80PqQPvclcX2NfrindfTwx5N+1OcnmUh56fGPod4nB+xR20er63wxCfYMpqoHV/Sa8gRffw1wGPQj178mvfXFABCcsDHnNH9SIj3sn7jBhJL1rEhaToFZUEyKOG+wMvUDJrBusyLKa8OMbpHCWdGP2dB8nkM7NuXk/2bbJCnH2fDAGDXcjuTY2mefbPe8A97abtgBTxzmg3PUJUTag6v84EYOM22tydm2A+ir4f9wJqo/eANmAI/eNVOALbtMzt/TDQCpTthwpW2HOvesU1L2afA1X+HA9vsl8e382wQtMYXsKFaWWT79g+ZAbtXwJb5dh6c2nJe/759feFqyBhpQ+TAdhtELRIbJjWV9kMdDcHEq+2qwvX2te1ZbZe3JiHdhlvoYJPlvW1gR1voaZScab9ICzfYsh4JT5wN3NrQrf1dlm+PyHwB+0UVrmocgs0ex2cfKxK0r7dWINUGtIgNbF/ACcKwXVexB4q32S/YpL42xIs22i/klvhTbSUiGoaDRc6YDKk/AoxPsqHu9dnHqiy263x+5wFqc0/sF1B8ov1CLtlp378JafZ1hqvt6w5V2nI3Ha8oHvuajbFlNca+T0y0+f/iqjdg2Bnt/Y80fhoN8NgTjkRZlV9KWVWIN1bks3hLEcFwlHEDUpkypBdbCw8yYVAqJx+fwYPvfcu3u8uorImQHPBRVFH/IU32+/j0p6eQkexv9PjGGNs9qSzfBnBievNChKpg1Wv2A9Yry9beX73ahvKsu2DmT+xUvP0nNb+eKdiaVvle2+QDdlbHz5weqwNyYNR5tkbe9FxD6a76Wl6e086flm1Dfu9auz7nehv+tWoq7bbBcltL7jPSniNY9Kj9UIcq7Qd98jX2C8bjszXkcNCO0E0ZYAOibLcd7BXoCef91o4fqHuOg/YLK5Bqa3OhKjt7ZaTG3m/LJ3Z5/0n2RHDJDlvWCVfacFn9mi1bWrbdrleWnc8HbDkKVtqf1EF2XW2tMFxtQywSskETjdiy98qy+732S7kloWqn5u4c9UTCNsiD5XBgh/0fxQXs0VrqILudMbY8oUr7RRAXaP3xa0XCjcdghKrskUpcD1v+aNgGc3xS431qjH1t8Yn2ecI19vUc67ER0Wj94D1vvH2Otk6wR8L2izgasf+H+MS293MbNMDVYdm5v5Il2/bTJ9nPjc8v53uTB/DD6VkcrAmT2iOOe+etoSIY4c0fnUSP+MPsJRKssO36w85q/EFsr71r7Ynf2p48Sn3HaYCrI/bAO9/yp8+3NVqW7PdRHgzzw+lD+OVFtl/81zsO8PLSnZRW1XDveWMYnN7KiUel1GFpLcC1H7g6pNvPGIbXK4zsl0zPHvFsKazgvPH9+fPnW3lm0TZCkShVoQjzcneT5PchwPm//5xrTxzCScf35oShafV90pVSx4zWwNURC4Yj/Pu8tczL3U04GuVHpxzPLbOzKSqv4aevr2T59mKiBk4dkcEPThjCkPQEBqclEIjTgTlKHQ5tQlEdpiIYpiYcJS0xvtnyV5bt5PGPNnKwpr43wdDeiQzvm8Se0moyU3tw1ti+bC08SEUwTFZ6IldMHaQhr1QDGuCqy5RXh9i8r4KdxZVsL6pkze5SthRWkJkaYH1BOfsP1uD1CH6fh8qaCHPHZfK7KyeRX1JFv9QAAmwpPEh2RiJxXh08rGKPtoGrLpMciGPS4F5MGtz8utc14SjrCso4rk8SifFenl64lV+/v55l24spLA+SEvAR7/NSVBHkxOx0/njNFAI+L/E+DXKlNMBVl4r3eZgwqGfd3zfPymZfeZC1u0v551nZbNhTTmUowrA+Sfz+081M+eVHhCKGkf2SOWNUX/qmBpg0qCdj+qfoiVIVczTAVbciIvzivNEtrpuenc6Ha/eS5PeyaHMRv5+/uW7dgJ49OHN0XxLivRwMhpk9IoPBaYnEez3anVF9Z2kbuHKtUCRKYXmQzzcX8eHaPSzcVEQ0aojzeqgK1Z80/d7kAdw8Kxufx8O3BWWEI1FG9EtmdKbW2pU76ElM9Z0XDEfweTyEo1EWb9lPaVWIdQXlPLNoK5Fo8/f5cRmJZKUnsjq/lJGZKZwwNI3BaQnMGpZBasKRDXlWqiNogKuYtbWwgrW7y6gJ25p3IM7Dih0lvPLVTkoqQ4wbmMrq/FK2FtoJpHrEeTl1ZAaBOC/rCsopqwoxd3wml04ZyLA+SWwprCAUMaQlxtMzIQ6/T7s8qo6lAa7UIRwMhtm4t5yXlu5k+fZiguEox2Uk4fd5+GxjIeGoITngo7y68YyAvZPi6ZsSoLw6TO+keGYNz2D28Ayy0hOpDEWoDIYRETKS/aT2aFyzN8ZQWBHE5/FgjCHvQBXHZSSSHNAjAFVPA1ypo1BUEeSt3N2sKygjJ6sXyYE4ig/WUHywht0lVewtqyalRxzb91eyaldJqxcHGtCzB4PSelAVilIZDLOvPEhpVePpZZP8Ps6fkElGcoDBaQmMHZDC8RlJfJNXQu7OEi6dMhCPR/h0/V4G9ExgSHoCPo+wcW8FiX4v4wakatv+d4wGuFKdpPhgDV9sLqKwPEiS30eC30skathTWs2q/FL2llaT4PeRGO8lLTGeYX3sdLARA5mpAd5fs4f56/dREayv6fs8Qthpx89I9mOMaTTlb0OjMlMY2juBg8EIecWVJAd8pCXGE+f1MG1oGrOGZ3DgYA0vOkcaY/qnMjWrF2P6pxI1htX5pazfU07vpHj2lQfZtLecUZkpBHxedpdW8cPpQ5gzph/RqLHXZGjjy2Kf88UWiPNSVRPB65FmffjLqkPMX7+PVbtKuTxnECP6JVMRDJMY7z3iL6LVu0p5edkO7j57VIeczzDGdOqXpAa4Ui4TiRq2FR1k7e5Svt1dxuD0BEb2S+beeWvpEeflp2eNoLImTEFpNcFQlOyMRHYdqOKt3N0UV9YQ7/UwJD2BimCYA5U1VAYjbC2qv1BEkt/HzGG9Wb+nnG1FjS8gMaBnDw5U1pDaI47hfZPreu8kxPvIL6miT7KfwoogAgTivCTEexndP5XEeC9fbtnPwF49SPT7WLatmPTEeCYO6smiTUUYDIPTEupG1FbWRNhZXAnY6bs9IvRLCZBfUkXvpHhSAnHkl1ThEWFo70RunpXNoLQEqmoiVNaEqQpFqKyxP1U1YXrE+zgxO53rnlvGvvIgU4b04pFLx1NUUUNBaRW9EuIZNyCVdXvK+NvyXSzcWMh1J2Vxw8lDWb+njJRAHDWRKGvzywhGovRKiGNYn2T+NzefNfmlTBrUk8Vb97Myr5TJQ3py8vG9GT+wJ2XVIfaVBQmGo5w3PhOAd1YVMH/DPvIPVBE1hv+8fCInHtfCnPntoAGulGLzvnLW5JeRHPCRMyStrnZaVBFk094KfF4hKz2x2cU7atWEozz3xTbW7ylnYK8eAFTVRCivDvNN3gEqqsOceFxv8g5Usr8iyNzx/fnW+QKaM6YfgTgvO/YfJOrkjs/rYWTfZKYfl05270SenL+FvWXVjOyXzPb9lVSFwgzoaZ9nwYZCNu2raNfrTIj3cuupx/PYhxtooQMSAInxXsb0T2XZ9uK6a1C0RgSyeyeypfAgg9J6MHt4Bit2lPBtQVmL29Y+1ujMFEZmJuPzCDecPJSR/Y7s2qga4EopV4tGDUu27acmbI8EEuK9dbX/hHgvPeK95BVX8tdlecwensGs4Rks21bM9v0HyUwN0Nep2a8rKGNkv2SmZqWR5Pfx1srdbNhTzqTBvaissSecxw1IJdHvZV+ZHRU8ZUgax/dJoqw6RGK8D6/HNp/srwiycW8FaYnxZCT7CYYjvPbVLnxe4YIJ/RmUdmwGkWmAK6WUS7UW4DojkFJKuZQGuFJKuZQGuFJKuZQGuFJKuZQGuFJKuZQGuFJKuZQGuFJKuZQGuFJKuVSnDuQRkUJgxxHevTdQdAyLc6x013JB9y2bluvwdNdyQfct23etXEOMMRlNF3ZqgB8NEVne0kikrtZdywXdt2xarsPTXcsF3bdssVIubUJRSimX0gBXSimXclOAP93VBWhFdy0XdN+yabkOT3ctF3TfssVEuVzTBq6UUqoxN9XAlVJKNaABrpRSLuWKABeRs0Vkg4hsFpG7u7Acg0Rkvoh8KyJrReR2Z/l9IpIvIrnOz7ldULbtIrLaef7lzrI0EflIRDY5v3t1cplGNNgnuSJSJiJ3dNX+EpFnRWSfiKxpsKzFfSTWE857bpWITO7kcj0iIuud5/67iPR0lmeJSFWDffdUJ5er1f+diPyrs782iMhZnVyuVxuUabuI5DrLO3N/tZYPHfceM8Z06x/AC2wBsoF4YCUwuovKkglMdm4nAxuB0cB9wE+7eD9tB3o3WfYb4G7n9t3Aw138f9wDDOmq/QXMAiYDaw61j4BzgfcBAaYDSzu5XHMAn3P74Qblymq4XRfsrxb/d87nYCXgB4Y6n1lvZ5WryfrHgHu7YH+1lg8d9h5zQw18GrDZGLPVGFMDvAJc2BUFMcYUGGNWOLfLgXXAgK4oSztdCDzv3H4euKjrisLpwBZjzJGOxD1qxpiFQHGTxa3towuBF4y1BOgpIpmdVS5jzIfGmLDz5xJgYEc89+GWqw0XAq8YY4LGmG3AZuxnt1PLJSICXA78tSOeuy1t5EOHvcfcEOADgLwGf++iG4SmiGQBk4ClzqJ/cQ6Dnu3spgqHAT4Uka9F5GZnWV9jTIFzew/QtwvKVev7NP5QdfX+qtXaPupO77sbsDW1WkNF5BsR+UxEZnZBeVr633WX/TUT2GuM2dRgWafvryb50GHvMTcEeLcjIknAG8Adxpgy4L+B44CJQAH2EK6znWyMmQycA9wqIrMarjT2mK1L+oyKSDxwAfA3Z1F32F/NdOU+ao2I3AOEgZecRQXAYGPMJOBO4GURSenEInXL/10DV9K4otDp+6uFfKhzrN9jbgjwfGBQg78HOsu6hIjEYf85Lxlj3gQwxuw1xkSMMVHgGTro0LEtxph85/c+4O9OGfbWHpI5v/d1drkc5wArjDF7nTJ2+f5qoLV91OXvOxG5DjgPuMr54OM0Uex3bn+NbWse3lllauN/1x32lw/4HvBq7bLO3l8t5QMd+B5zQ4B/BQwTkaFOTe77wFtdURCnfe3PwDpjzH82WN6w3epiYE3T+3ZwuRJFJLn2NvYE2BrsfrrW2exaYF5nlquBRrWirt5fTbS2j94CrnF6CkwHShscBnc4ETkb+H/ABcaYygbLM0TE69zOBoYBWzuxXK39794Cvi8ifhEZ6pRrWWeVy3EGsN4Ys6t2QWfur9bygY58j3XG2dljcHb3XOwZ3S3APV1YjpOxhz+rgFzn51zgL8BqZ/lbQGYnlysb2wNgJbC2dh8B6cAnwCbgYyCtC/ZZIrAfSG2wrEv2F/ZLpAAIYdsb/6m1fYTtGfCk855bDeR0crk2Y9tHa99nTznbXuL8j3OBFcD5nVyuVv93wD3O/toAnNOZ5XKW/w9wS5NtO3N/tZYPHfYe06H0SinlUm5oQlFKKdUCDXCllHIpDXCllHIpDXCllHIpDXCllHIpDXCllHIpDXCllHKp/w+GhntoFyBi/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(np.arange(epochs), train_loss, label='train loss')\n",
    "plt.plot(np.arange(epochs), test_loss, label='val loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b497b990-bad4-4a60-a567-4fb855089f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    torch.save(model.state_dict(), \"model/best_numGNN2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea24b96-0bb4-4825-8511-025d23c07178",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2b58b-befe-46af-9ac2-cb889717fceb",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e876ea1-3a3e-4f5d-bfa6-420b19b1c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 65625\n",
      "Number of test graphs: 16237\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pickle.load(open('data/train/graph_concat.pkl', 'rb'))\n",
    "test_dataset = pickle.load(open('data/test/graph_concat.pkl', 'rb'))\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b15ad756-929a-4341-a850-2443109b32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = []\n",
    "test_feat = []\n",
    "\n",
    "for train_data in train_dataset:\n",
    "    train_feat.append(train_data.x_feat)\n",
    "    \n",
    "for test_data in test_dataset:\n",
    "    test_feat.append(test_data.x_feat)\n",
    "\n",
    "\n",
    "train_feat = torch.cat(train_feat, axis=0)\n",
    "test_feat = torch.cat(test_feat, axis=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_feat = torch.from_numpy(sc.fit_transform(train_feat))\n",
    "test_feat = torch.from_numpy(sc.transform(test_feat))\n",
    "\n",
    "for i, data in enumerate(train_dataset):\n",
    "    data.x_feat = train_feat[i].unsqueeze(0)\n",
    "\n",
    "for i, data in enumerate(test_dataset):\n",
    "    data.x_feat = test_feat[i].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8c5717e-5674-4121-966a-c3babae5999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea2ece4-05e7-469a-a880-0eb27fe99531",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "160f8d28-db63-4202-ab0f-64e030d9b4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (g): SAGE(\n",
       "    (conv1): SAGEConv(9, 32)\n",
       "    (conv2): SAGEConv(32, 32)\n",
       "    (conv3): SAGEConv(32, 32)\n",
       "    (conv4): SAGEConv(32, 32)\n",
       "    (lin): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (lin1): Linear(in_features=21, out_features=32, bias=True)\n",
       "    (lin2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin4): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (lin): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (cross): CrossNet()\n",
       "  (mlp_cross): MLP(\n",
       "    (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp_out): MLP(\n",
       "    (lin1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_node_features = 9\n",
    "hidden_channels = 32\n",
    "num_feats = 21\n",
    "num_classes = 1\n",
    "num_attr = 3\n",
    "\n",
    "model = Net(num_node_features, num_feats, hidden_channels, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #0.001\n",
    "criterion = torch.nn.L1Loss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.85, patience=3, min_lr=0.000001)\n",
    "\n",
    "model.load_state_dict(torch.load('model/best_SAGE_concat.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f3c94-3bb9-44c0-acc4-8e582aeac2d3",
   "metadata": {},
   "source": [
    "## Evaluate Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35b0b9ff-5788-47ec-a798-27ac83cac519",
   "metadata": {},
   "outputs": [],
   "source": [
    "mofname = []\n",
    "co2_select = []\n",
    "\n",
    "for data in test_loader:  # Iterate in batches over the training dataset.\n",
    "    data.to(device)\n",
    "    \n",
    "    out = model(data.x, data.edge_index, data.batch, data.x_feat.float()).cpu().detach().numpy()\n",
    "    mofname.append(data.mofname)\n",
    "    co2_select.append(out)\n",
    "    \n",
    "mofname = np.concatenate(mofname)\n",
    "co2_select = np.concatenate(co2_select).flatten()\n",
    "\n",
    "cut_mof_unit = lambda x: x.split('_')[-1]\n",
    "id_ = np.array(list(map(cut_mof_unit, mofname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9db2345b-cf9e-43c3-bc68-67993c405a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id': id_, 'CO2_working_capacity [mL/g]': co2_select}\n",
    "\n",
    "df_inference = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7435fc60-3757-427b-a8c1-bffe25e4fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgboost = pd.read_csv('xgboost_submission.csv')\n",
    "df_xgboost = df_xgboost.set_index('id')\n",
    "\n",
    "df_xgboost.loc[df_inference.id.values.astype(int)] = np.expand_dims(df_inference['CO2_working_capacity [mL/g]'].values, axis=1)\n",
    "df_xgboost = df_xgboost.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efb0471b-287b-4ffa-b888-f363e344f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgboost.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fb8d3ae-2411-4ea2-856f-fe7cd2c5d9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>CO2_working_capacity [mL/g]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68614</td>\n",
       "      <td>216.854218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68615</td>\n",
       "      <td>71.110825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68616</td>\n",
       "      <td>63.646053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68617</td>\n",
       "      <td>55.750607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68618</td>\n",
       "      <td>65.396301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16995</th>\n",
       "      <td>85609</td>\n",
       "      <td>-7.181580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>85610</td>\n",
       "      <td>1.273886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16997</th>\n",
       "      <td>85611</td>\n",
       "      <td>-0.719142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16998</th>\n",
       "      <td>85612</td>\n",
       "      <td>-1.186282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999</th>\n",
       "      <td>85613</td>\n",
       "      <td>-4.660836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  CO2_working_capacity [mL/g]\n",
       "0      68614                   216.854218\n",
       "1      68615                    71.110825\n",
       "2      68616                    63.646053\n",
       "3      68617                    55.750607\n",
       "4      68618                    65.396301\n",
       "...      ...                          ...\n",
       "16995  85609                    -7.181580\n",
       "16996  85610                     1.273886\n",
       "16997  85611                    -0.719142\n",
       "16998  85612                    -1.186282\n",
       "16999  85613                    -4.660836\n",
       "\n",
       "[17000 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccfd22f-5ebe-43cd-9621-06a0ddcd6f37",
   "metadata": {},
   "source": [
    "## Create Latent Space for AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3da5e7a7-88cc-46b5-a7e5-2aca58e7215b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (g): SAGE(\n",
       "    (conv1): SAGEConv(9, 32)\n",
       "    (conv2): SAGEConv(32, 32)\n",
       "    (conv3): SAGEConv(32, 32)\n",
       "    (conv4): SAGEConv(32, 32)\n",
       "    (lin): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (lin1): Linear(in_features=21, out_features=32, bias=True)\n",
       "    (lin2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin4): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (lin): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (cross): CrossNet()\n",
       "  (mlp_cross): MLP(\n",
       "    (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp_out): MLP(\n",
       "    (lin1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (out): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.out=nn.Sequential(*list(model.out.children())[:-1])\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91628a15-bd6f-4df6-812e-d86634bc8e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 100\n",
      "done: 200\n",
      "done: 300\n",
      "done: 400\n",
      "done: 500\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "train_x = []\n",
    "train_y = []\n",
    "train_mofname = []\n",
    "\n",
    "for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "    data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch, data.x_feat.float()).cpu().detach().numpy()\n",
    "    x_feat = data.x_feat.cpu().detach().numpy()\n",
    "\n",
    "    out = np.concatenate((out, x_feat), axis=1)\n",
    "    train_x.append(out)\n",
    "    train_y.append(data.y.cpu().detach().numpy())\n",
    "    train_mofname.append(data.mofname)\n",
    "    c=c+1\n",
    "    if(c%100==0):\n",
    "        print('done:',c)\n",
    "\n",
    "train_x = np.concatenate(train_x, axis=0)\n",
    "train_y = np.concatenate(train_y, axis=0)\n",
    "train_mofname = np.concatenate(train_mofname, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00dc8e64-9c19-401b-864d-13b4539fa5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 100\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "test_x = []\n",
    "test_mofname = []\n",
    "\n",
    "for data in test_loader:  # Iterate in batches over the training dataset.\n",
    "    data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch, data.x_feat.float()).cpu().detach().numpy()\n",
    "    x_feat = data.x_feat.cpu().detach().numpy()\n",
    "\n",
    "    out = np.concatenate((out, x_feat), axis=1)\n",
    "    test_x.append(out)\n",
    "    test_mofname.append(data.mofname)\n",
    "    c=c+1\n",
    "    if(c%100==0):\n",
    "        print('done:',c)\n",
    "\n",
    "test_x = np.concatenate(test_x, axis=0)\n",
    "test_mofname = np.concatenate(test_mofname, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8b951cd-46fd-4d9a-b530-cdf6e6cfc44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_x)\n",
    "test_df = pd.DataFrame(test_x)\n",
    "\n",
    "train_df['target'] = train_y.flatten()\n",
    "train_df['mofname'] = train_mofname.flatten()\n",
    "test_df['mofname'] = test_mofname.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30c2f8-7470-484a-853b-d5d47037c187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((65625,), (65625,))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mofname.shape, co2_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4cf83f50-e2a7-4ab8-8c32-aec32212f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'mofname': mofname, 'CO2_working_capacity [mL/g]': co2_select}\n",
    "\n",
    "df_inference = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c59a166-f0e8-438e-95fb-49c5fa3af489",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/train/Latent_SAGE_feat_train.csv',index=False)\n",
    "# test_df.to_csv('data/test/Latent_SAGE_feat_mofname_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12171eea-b02a-49f7-b95d-493d132fec69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mofname</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mof_unit_1</td>\n",
       "      <td>89.982483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mof_unit_2</td>\n",
       "      <td>95.660843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mof_unit_4</td>\n",
       "      <td>160.562881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mof_unit_5</td>\n",
       "      <td>88.837372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mof_unit_6</td>\n",
       "      <td>66.148277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65620</th>\n",
       "      <td>mof_unit_68609</td>\n",
       "      <td>10.749875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65621</th>\n",
       "      <td>mof_unit_68610</td>\n",
       "      <td>0.032699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65622</th>\n",
       "      <td>mof_unit_68611</td>\n",
       "      <td>0.301270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65623</th>\n",
       "      <td>mof_unit_68612</td>\n",
       "      <td>0.037428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65624</th>\n",
       "      <td>mof_unit_68613</td>\n",
       "      <td>0.024978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65625 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              mofname     predict\n",
       "0          mof_unit_1   89.982483\n",
       "1          mof_unit_2   95.660843\n",
       "2          mof_unit_4  160.562881\n",
       "3          mof_unit_5   88.837372\n",
       "4          mof_unit_6   66.148277\n",
       "...               ...         ...\n",
       "65620  mof_unit_68609   10.749875\n",
       "65621  mof_unit_68610    0.032699\n",
       "65622  mof_unit_68611    0.301270\n",
       "65623  mof_unit_68612    0.037428\n",
       "65624  mof_unit_68613    0.024978\n",
       "\n",
       "[65625 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99189d-4947-4149-8e7b-9b48a564f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.join(smiles.set_index('MOFname'), on='MOFname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444203d4-0732-4236-b4b7-07da0cd33871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mofname</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>target</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mof_unit_1</td>\n",
       "      <td>13.082182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.234041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.093610</td>\n",
       "      <td>-0.661012</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.703202</td>\n",
       "      <td>-0.921714</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.038235</td>\n",
       "      <td>0.578007</td>\n",
       "      <td>105.284500</td>\n",
       "      <td>89.982483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mof_unit_2</td>\n",
       "      <td>13.931372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.117464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.737443</td>\n",
       "      <td>-0.570913</td>\n",
       "      <td>1.806121</td>\n",
       "      <td>2.994665</td>\n",
       "      <td>3.582469</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.830237</td>\n",
       "      <td>101.224777</td>\n",
       "      <td>95.660843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mof_unit_4</td>\n",
       "      <td>23.637325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.421621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252887</td>\n",
       "      <td>-0.325378</td>\n",
       "      <td>1.491358</td>\n",
       "      <td>0.498605</td>\n",
       "      <td>0.351208</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.019897</td>\n",
       "      <td>0.161896</td>\n",
       "      <td>187.626007</td>\n",
       "      <td>160.562881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mof_unit_5</td>\n",
       "      <td>12.911613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.052068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.100921</td>\n",
       "      <td>-0.664341</td>\n",
       "      <td>-0.711980</td>\n",
       "      <td>-0.980542</td>\n",
       "      <td>0.155374</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.362969</td>\n",
       "      <td>79.209999</td>\n",
       "      <td>88.837372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mof_unit_6</td>\n",
       "      <td>9.515944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.403864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670308</td>\n",
       "      <td>0.403943</td>\n",
       "      <td>1.491358</td>\n",
       "      <td>-0.425862</td>\n",
       "      <td>0.253291</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.075191</td>\n",
       "      <td>-0.390945</td>\n",
       "      <td>55.786961</td>\n",
       "      <td>66.148277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65620</th>\n",
       "      <td>mof_unit_68609</td>\n",
       "      <td>1.574537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.899714</td>\n",
       "      <td>0.337198</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.570856</td>\n",
       "      <td>-0.787317</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.703202</td>\n",
       "      <td>0.351208</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.030042</td>\n",
       "      <td>0.219719</td>\n",
       "      <td>-12.943652</td>\n",
       "      <td>10.749875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65621</th>\n",
       "      <td>mof_unit_68610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528994</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.503912</td>\n",
       "      <td>-0.787317</td>\n",
       "      <td>1.806121</td>\n",
       "      <td>2.809771</td>\n",
       "      <td>2.505382</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.146773</td>\n",
       "      <td>-1.755135</td>\n",
       "      <td>-12.985581</td>\n",
       "      <td>0.032699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65622</th>\n",
       "      <td>mof_unit_68611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399044</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.570856</td>\n",
       "      <td>-0.787317</td>\n",
       "      <td>-0.082455</td>\n",
       "      <td>0.221265</td>\n",
       "      <td>0.155374</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.122184</td>\n",
       "      <td>0.219719</td>\n",
       "      <td>-13.187635</td>\n",
       "      <td>0.301270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65623</th>\n",
       "      <td>mof_unit_68612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523465</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.570856</td>\n",
       "      <td>-0.787317</td>\n",
       "      <td>-0.082455</td>\n",
       "      <td>-0.703202</td>\n",
       "      <td>-0.530045</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.151858</td>\n",
       "      <td>-1.755135</td>\n",
       "      <td>15.672698</td>\n",
       "      <td>0.037428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65624</th>\n",
       "      <td>mof_unit_68613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539275</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.473643</td>\n",
       "      <td>-0.787317</td>\n",
       "      <td>-0.711980</td>\n",
       "      <td>-0.240968</td>\n",
       "      <td>-0.432128</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.169076</td>\n",
       "      <td>-1.755135</td>\n",
       "      <td>3.144708</td>\n",
       "      <td>0.024978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65625 rows  280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              mofname          0    1    2    3    4    5         6  \\\n",
       "0          mof_unit_1  13.082182  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "1          mof_unit_2  13.931372  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "2          mof_unit_4  23.637325  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "3          mof_unit_5  12.911613  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "4          mof_unit_6   9.515944  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "...               ...        ...  ...  ...  ...  ...  ...       ...   \n",
       "65620  mof_unit_68609   1.574537  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "65621  mof_unit_68610   0.000000  0.0  0.0  0.0  0.0  0.0  0.578905   \n",
       "65622  mof_unit_68611   0.000000  0.0  0.0  0.0  0.0  0.0  0.563143   \n",
       "65623  mof_unit_68612   0.000000  0.0  0.0  0.0  0.0  0.0  0.582178   \n",
       "65624  mof_unit_68613   0.000000  0.0  0.0  0.0  0.0  0.0  0.578771   \n",
       "\n",
       "               7         8  ...       269       270       271       272  \\\n",
       "0      14.234041  0.000000  ... -1.093610 -0.661012 -0.397217 -0.703202   \n",
       "1      15.117464  0.000000  ... -0.737443 -0.570913  1.806121  2.994665   \n",
       "2      25.421621  0.000000  ... -0.252887 -0.325378  1.491358  0.498605   \n",
       "3      14.052068  0.000000  ... -1.100921 -0.664341 -0.711980 -0.980542   \n",
       "4      10.403864  0.000000  ...  0.670308  0.403943  1.491358 -0.425862   \n",
       "...          ...       ...  ...       ...       ...       ...       ...   \n",
       "65620   1.899714  0.337198  ... -1.570856 -0.787317 -0.397217 -0.703202   \n",
       "65621   0.000000  0.528994  ... -1.503912 -0.787317  1.806121  2.809771   \n",
       "65622   0.000000  0.399044  ... -1.570856 -0.787317 -0.082455  0.221265   \n",
       "65623   0.000000  0.523465  ... -1.570856 -0.787317 -0.082455 -0.703202   \n",
       "65624   0.000000  0.539275  ... -1.473643 -0.787317 -0.711980 -0.240968   \n",
       "\n",
       "            273       274       275       276      target     predict  \n",
       "0     -0.921714  0.359778 -0.038235  0.578007  105.284500   89.982483  \n",
       "1      3.582469  0.359778  0.031265  0.830237  101.224777   95.660843  \n",
       "2      0.351208  0.359778 -0.019897  0.161896  187.626007  160.562881  \n",
       "3      0.155374  0.359778  0.007900  0.362969   79.209999   88.837372  \n",
       "4      0.253291  0.359778 -0.075191 -0.390945   55.786961   66.148277  \n",
       "...         ...       ...       ...       ...         ...         ...  \n",
       "65620  0.351208  0.359778 -0.030042  0.219719  -12.943652   10.749875  \n",
       "65621  2.505382 -2.779491 -0.146773 -1.755135  -12.985581    0.032699  \n",
       "65622  0.155374  0.359778 -0.122184  0.219719  -13.187635    0.301270  \n",
       "65623 -0.530045 -2.779491 -0.151858 -1.755135   15.672698    0.037428  \n",
       "65624 -0.432128 -2.779491 -0.169076 -1.755135    3.144708    0.024978  \n",
       "\n",
       "[65625 rows x 280 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dfdf.reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMLCC_CUDA",
   "language": "python",
   "name": "tmlcc_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
