{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08d8716-5267-467e-8fbf-f956bfbeeaca",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38601030-b6e2-4ea2-9844-49c943334f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GINConv, GINEConv, GCNConv, GraphConv, SAGEConv, ChebConv, global_add_pool, global_mean_pool\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b376f8b-82c5-40bb-ba48-0dedd85f7591",
   "metadata": {},
   "source": [
    "# Run Pytorch on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f424459d-d858-4255-9250-1f940079f474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_pytorch_version(version):\n",
    "    return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "    return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34ea2e-7a10-4e27-8fe3-29fb7b0132a6",
   "metadata": {},
   "source": [
    "# DataSet & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac311ce8-9e84-4c67-8d4e-2d8f8659cbf2",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6066e87-d49c-4207-8c46-cb7983177cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topo_0                                           0\n",
      "topo_1                                           0\n",
      "topo_2                                           0\n",
      "topo_3                                           0\n",
      "topo_4                                           0\n",
      "topo_5                                           0\n",
      "topo_6                                           0\n",
      "topo_7                                           0\n",
      "topo_8                                           0\n",
      "topo_9                                           0\n",
      "MOFname                                          0\n",
      "volume [A^3]                                     0\n",
      "weight [u]                                       0\n",
      "density [g/cm^3]                                 0\n",
      "surface_area [m^2/g]                             0\n",
      "void_fraction                                    0\n",
      "void_volume [cm^3/g]                             0\n",
      "functional_groups                                0\n",
      "metal_linker                                     0\n",
      "organic_linker1                                  0\n",
      "organic_linker2                                  0\n",
      "catalog CO2/N2                                   0\n",
      "CO2/N2_selectivity                               0\n",
      "heat_adsorption_CO2_P0.15bar_T298K [kcal/mol]    0\n",
      "Smiles                                           0\n",
      "dtype: int64\n",
      "(17000, 25)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/test/clean_test.csv')\n",
    "smiles = pd.read_csv('data/test/smiles_test.csv')\n",
    "data = df.join(smiles.set_index('MOFname'), on='MOFname')\n",
    "\n",
    "data = data.dropna(subset=['Smiles'])\n",
    "data = data.reset_index(drop=True)\n",
    "mask = data['surface_area [m^2/g]']>0\n",
    "mask = mask.values\n",
    "data = data[mask]\n",
    "print(data.isnull().sum())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e11f0e34-b17a-4f43-9a1a-0b4f0d2fc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map = {\n",
    "    'atomic_num':\n",
    "    list(range(0, 119)),\n",
    "    'chirality': [\n",
    "        'CHI_UNSPECIFIED',\n",
    "        'CHI_TETRAHEDRAL_CW',\n",
    "        'CHI_TETRAHEDRAL_CCW',\n",
    "        'CHI_OTHER',\n",
    "    ],\n",
    "    'degree':\n",
    "    list(range(0, 11)),\n",
    "    'formal_charge':\n",
    "    list(range(-5, 7)),\n",
    "    'num_hs':\n",
    "    list(range(0, 9)),\n",
    "    'num_radical_electrons':\n",
    "    list(range(0, 5)),\n",
    "    'hybridization': [\n",
    "        'UNSPECIFIED',\n",
    "        'S',\n",
    "        'SP',\n",
    "        'SP2',\n",
    "        'SP3',\n",
    "        'SP3D',\n",
    "        'SP3D2',\n",
    "        'OTHER',\n",
    "    ],\n",
    "    'is_aromatic': [False, True],\n",
    "    'is_in_ring': [False, True],\n",
    "}\n",
    "\n",
    "e_map = {\n",
    "    'bond_type': [\n",
    "        'misc',\n",
    "        'SINGLE',\n",
    "        'DOUBLE',\n",
    "        'TRIPLE',\n",
    "        'AROMATIC',\n",
    "    ],\n",
    "    'stereo': [\n",
    "        'STEREONONE',\n",
    "        'STEREOZ',\n",
    "        'STEREOE',\n",
    "        'STEREOCIS',\n",
    "        'STEREOTRANS',\n",
    "        'STEREOANY',\n",
    "    ],\n",
    "    'is_conjugated': [False, True],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc54ebf3-b60c-40b5-97cc-2d12fb058556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 10000\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "data_dict = []\n",
    "c = 1\n",
    "for _, line in data.iterrows():\n",
    "    mol = Chem.MolFromSmiles(line['Smiles'])\n",
    "    \n",
    "    if mol == None:\n",
    "        continue\n",
    "    \n",
    "    # Create Node Features\n",
    "    xs = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        x = []\n",
    "        x.append(x_map['atomic_num'].index(atom.GetAtomicNum()))\n",
    "        x.append(x_map['chirality'].index(str(atom.GetChiralTag())))\n",
    "        x.append(x_map['degree'].index(atom.GetTotalDegree()))\n",
    "        x.append(x_map['formal_charge'].index(atom.GetFormalCharge()))\n",
    "        x.append(x_map['num_hs'].index(atom.GetTotalNumHs()))\n",
    "        x.append(x_map['num_radical_electrons'].index(atom.GetNumRadicalElectrons()))\n",
    "        x.append(x_map['hybridization'].index(str(atom.GetHybridization())))\n",
    "        x.append(x_map['is_aromatic'].index(atom.GetIsAromatic()))\n",
    "        x.append(x_map['is_in_ring'].index(atom.IsInRing()))\n",
    "        xs.append(x)\n",
    "    x = torch.tensor(xs, dtype=torch.float).view(-1, 9)\n",
    "    \n",
    "    # Create Edge Features\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        e = []\n",
    "        e.append(e_map['bond_type'].index(str(bond.GetBondType())))\n",
    "        e.append(e_map['stereo'].index(str(bond.GetStereo())))\n",
    "        e.append(e_map['is_conjugated'].index(bond.GetIsConjugated()))\n",
    "\n",
    "        edge_indices += [[i, j], [j, i]]\n",
    "        edge_attrs += [e, e]\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices)\n",
    "    edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3)\n",
    "\n",
    "    # Sort indices.\n",
    "    if edge_index.numel() > 0:\n",
    "        perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n",
    "        edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n",
    "\n",
    "    x_feat = line.drop(['MOFname', 'surface_area [m^2/g]', 'functional_groups', 'Smiles']).values.astype(float) #, 'weight [u]', 'functional_groups', 'CO2_working_capacity [mL/g]'\n",
    "    x_feat = np.expand_dims(x_feat, axis=0)\n",
    "    x_feat = torch.tensor(x_feat)\n",
    "    y=torch.tensor([line['surface_area [m^2/g]']], dtype=torch.float).view(1, -1)\n",
    "    data_d = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, smiles=line['Smiles'], mofname=line['MOFname'], x_feat=x_feat) #, y=y\n",
    "    data_d.num_nodes = len(mol.GetAtoms())\n",
    "    data_list.append(data_d)\n",
    "    data_dict.append(line['MOFname'])\n",
    "    \n",
    "    if(c%10000==0):\n",
    "        print('done:',c)\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94bd46-dbbc-4b35-8136-4cbdf8b6630d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa0214f3-676a-4117-a494-237cab095356",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = pickle.load(open('data/graph_all_surface.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9e88aa4-e0cf-42cb-917c-f666281c8f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 54432\n",
      "Number of test graphs: 13609\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "datasets = data_list\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(datasets, test_size=0.2, random_state = 1, shuffle=True)\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c803d206-82dc-46b1-a86a-3dfbb2364d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = []\n",
    "test_feat = []\n",
    "\n",
    "for train_data in train_dataset:\n",
    "    train_feat.append(train_data.x_feat)\n",
    "    \n",
    "for test_data in test_dataset:\n",
    "    test_feat.append(test_data.x_feat)\n",
    "\n",
    "\n",
    "train_feat = torch.cat(train_feat, axis=0)\n",
    "test_feat = torch.cat(test_feat, axis=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_feat = torch.from_numpy(sc.fit_transform(train_feat))\n",
    "test_feat = torch.from_numpy(sc.transform(test_feat))\n",
    "\n",
    "for i, data in enumerate(train_dataset):\n",
    "    data.x_feat = train_feat[i].unsqueeze(0)\n",
    "\n",
    "for i, data in enumerate(test_dataset):\n",
    "    data.x_feat = test_feat[i].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "167bf06b-c717-477f-bfcb-251a0fc2bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "data_loader = DataLoader(datasets, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4fcce-2a3b-4868-94cf-9d941c63fded",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde77d00-5597-4195-9fa5-96acd3649773",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GINE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb7735e0-f581-4bd5-97a8-ca21042e8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, in_attr, dim, out_channels):\n",
    "        super(GINE, self).__init__()\n",
    "\n",
    "        self.attr1 = Sequential(Linear(in_attr, in_channels), ReLU())\n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(in_channels, dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr2 = Sequential(Linear(in_channels, dim), ReLU())\n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr3 = Sequential(Linear(dim, dim), ReLU())\n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr4 = Sequential(Linear(dim, dim), ReLU())\n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr5 = Sequential(Linear(dim, dim), ReLU())\n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.lin1 = Sequential(Linear(dim, dim), ReLU())\n",
    "        self.lin2 = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        edge_attr = self.attr1(edge_attr)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr2(edge_attr)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr3(edge_attr)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr4(edge_attr)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr5(edge_attr)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b59b767-83ce-4ca5-b20a-2fd757af246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(in_channels, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv4 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv5 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "        self.lin2 = Linear(dim, out_channels)\n",
    "        self.lin3 = Linear(out_channels, out_channels)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fafd152-2a49-4276-85de-06958a565f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, dim)\n",
    "        self.conv2 = GCNConv(dim, dim)\n",
    "        self.conv3 = GCNConv(dim, dim)\n",
    "        self.conv4 = GCNConv(dim, dim)\n",
    "        self.lin = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "789f7ab1-9725-4307-86d6-c64800f952e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, dim)\n",
    "        self.conv2 = SAGEConv(dim, dim)\n",
    "        self.conv3 = SAGEConv(dim, dim)\n",
    "        self.conv4 = SAGEConv(dim, dim)\n",
    "        self.lin = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ffba7f3-45e4-495d-a44d-8c9bd2fb90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(Graph, self).__init__()\n",
    "        self.conv1 = GraphConv(in_channels, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.conv3 = GraphConv(dim, dim)\n",
    "        self.conv4 = GraphConv(dim, dim)\n",
    "        self.lin = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377f1cd-b88e-4662-831b-4480b173f1b4",
   "metadata": {},
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a9fc53d-c55d-41a4-8083-e1aabf5fb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lin1 = Linear(in_channels, dim)\n",
    "        self.lin2 = Linear(dim, dim)\n",
    "        self.lin3 = Linear(dim, dim)\n",
    "        self.lin4 = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x).relu()\n",
    "#         x = self.lin3(x).relu()\n",
    "        x = self.lin4(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a19426-45fa-4120-bc5e-e46b44a7f5c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2dbed4-82bb-45f7-9cc1-ef577c1b1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, layer_num=2):\n",
    "        super(CrossNet, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.kernels = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "\n",
    "        for i in range(self.kernels.shape[0]):\n",
    "            nn.init.xavier_normal_(self.kernels[i])\n",
    "        for i in range(self.bias.shape[0]):\n",
    "            nn.init.zeros_(self.bias[i])\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_0 = inputs.unsqueeze(2)\n",
    "        x_l = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            xl_w = torch.tensordot(x_l, self.kernels[i], dims=([1], [0]))\n",
    "            dot_ = torch.matmul(x_0, xl_w)\n",
    "            x_l = dot_ + self.bias[i] + x_l\n",
    "        x_l = torch.squeeze(x_l, dim=2)\n",
    "        return x_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82eb29-3e47-45f7-afd4-3c28c6963fd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multi Input Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8e2a6a-ba42-4c4a-8de0-bf276fd3976b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, in_xs, in_attr, in_xfeats, dim, out_channels):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.gine = GINE(in_xs, in_attr, dim, 128)\n",
    "#         self.mlp_num = MLP(in_xfeats, dim, 128)\n",
    "#         self.bn = BatchNorm1d(256)\n",
    "#         self.lin = Sequential(Linear(256, 128), BatchNorm1d(128))\n",
    "#         self.lin2 = Sequential(Linear(128, 128), BatchNorm1d(128))\n",
    "#         # Deep & Cross Network\n",
    "#         self.crossnet = CrossNet(128)\n",
    "#         self.mlp_cross = MLP(128, 256, 128)\n",
    "        \n",
    "#         self.bn_cat = BatchNorm1d(256)\n",
    "#         self.mlp_cat = MLP(256, 256, 128)\n",
    "#         self.dropout = Dropout(p=0.5)\n",
    "#         self.out = Linear(128, out_channels) # 256\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_attr, batch, x_feat):\n",
    "#         x = self.gine(x, edge_index, edge_attr, batch)\n",
    "#         x_feat = self.mlp_num(x_feat)\n",
    "#         concat = torch.cat((x, x_feat),dim=1)\n",
    "#         x = self.bn(concat)\n",
    "#         x = self.lin(x)\n",
    "#         x = self.lin2(x)\n",
    "        \n",
    "#         # Deep & Cross Network\n",
    "#         hl = self.mlp_cross(x)\n",
    "#         xl = self.crossnet(x)\n",
    "#         x = torch.cat((xl, hl), dim=1)\n",
    "#         x = self.bn_cat(x)\n",
    "#         x = self.mlp_cat(x)\n",
    "#         x = self.dropout(x)\n",
    "#         out = self.out(x)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a339c47-6c9b-408d-82ee-77e1efefdebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class DCN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(DCN, self).__init__()\n",
    "#         # Deep & Cross Network\n",
    "#         self.crossnet = CrossNet(in_channels)\n",
    "#         self.mlp_cross = MLP(in_channels, 256, 128)\n",
    "        \n",
    "#         self.bn = BatchNorm1d(128+in_channels)\n",
    "#         self.mlp_cat = MLP(128+in_channels, 512, 256)\n",
    "#         self.bn_out = BatchNorm1d(256)\n",
    "#         self.dropout = Dropout(p=0.5)\n",
    "#         self.out = Linear(256, out_channels) # 256\n",
    "\n",
    "#     def forward(self, x):        \n",
    "#         # Deep & Cross Network\n",
    "#         hl = self.mlp_cross(x)\n",
    "#         xl = self.crossnet(x)\n",
    "#         x = torch.cat((xl, hl), dim=1)\n",
    "#         x = self.bn(x)\n",
    "#         x = self.mlp_cat(x)\n",
    "#         x = self.bn_out(x)\n",
    "# #         x = self.dropout(x)\n",
    "#         out = self.out(x)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d00365a0-505e-4adf-b703-f17774e8e674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_xs, in_xfeats, dim, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.g = SAGE(in_xs, dim, 128) # SAGE\n",
    "        self.mlp = MLP(in_xfeats, dim, 128)\n",
    "        self.lin = Linear(256, 128)\n",
    "        self.lin2 = Linear(128, 128)\n",
    "        self.cross = CrossNet(128, layer_num=2)\n",
    "        self.mlp_cross = MLP(128, 128, 128)\n",
    "        self.mlp_out = MLP(256, 256, 256)\n",
    "        self.out = Linear(256, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, x_feat):\n",
    "        x = self.g(x, edge_index, batch)\n",
    "        x_feat = self.mlp(x_feat)\n",
    "        concat = torch.cat((x, x_feat), dim=1)\n",
    "        x = self.lin(concat)\n",
    "        x = F.dropout(x, p=0.3, training=self.training) # SAGE: 0.3\n",
    "        x = self.lin2(x).relu()\n",
    "        \n",
    "        cross = self.cross(x)\n",
    "        mlp_cross = self.mlp_cross(x)\n",
    "        concat2 = torch.cat((cross, mlp_cross), dim=1)\n",
    "        \n",
    "        x = self.mlp_out(concat2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training) # SAGE: 0.3\n",
    "        out = self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14c8bc7f-07c8-43d0-94c8-3b60f56ce02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_xs, in_xfeats, in_attr, dim, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.g = GINE(in_xs, in_attr, dim, 128) # SAGE\n",
    "        self.mlp = MLP(in_xfeats, dim, 128)\n",
    "        self.lin = Linear(256, 128)\n",
    "        self.lin2 = Linear(128, 128)\n",
    "        self.cross = CrossNet(128, layer_num=2)\n",
    "        self.mlp_cross = MLP(128, 128, 128)\n",
    "        self.mlp_out = MLP(256, 256, 256)\n",
    "        self.out = Linear(256, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, x_feat):\n",
    "        x = self.g(x, edge_index, edge_attr, batch)\n",
    "        x_feat = self.mlp(x_feat)\n",
    "        concat = torch.cat((x, x_feat), dim=1)\n",
    "        x = self.lin(concat)\n",
    "        x = F.dropout(x, p=0.3, training=self.training) # SAGE: 0.3\n",
    "        x = self.lin2(x).relu()\n",
    "        \n",
    "        cross = self.cross(x)\n",
    "        mlp_cross = self.mlp_cross(x)\n",
    "        concat2 = torch.cat((cross, mlp_cross), dim=1)\n",
    "        \n",
    "        x = self.mlp_out(concat2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training) # SAGE: 0.3\n",
    "        out = self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436d70d-f407-4829-bc1a-70ff24aef54d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "711f76cb-8705-4ebf-a0ce-2b21ac50f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "    c=0\n",
    "    correct=0\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, data.x_feat.float())  # Perform a single forward pass. , data.edge_attr.float()\n",
    "    \n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        c=c+1\n",
    "        correct+=loss.cpu().detach().numpy()\n",
    "    return correct/c\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    c=0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, data.x_feat.float()) # , data.edge_attr.float()\n",
    "    \n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        correct += loss.cpu().detach().numpy()  # Check against ground-truth labels.\n",
    "        c=c+1\n",
    "    return correct / c  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08571f39-3903-4f7a-8fa8-769e373c125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features = 9\n",
    "hidden_channels = 64 # SAGE: 32\n",
    "num_feats = 21\n",
    "num_classes = 1\n",
    "num_attr = 3\n",
    "\n",
    "model = Net(num_node_features, num_feats, hidden_channels, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #0.001\n",
    "criterion = torch.nn.L1Loss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.85, patience=3, min_lr=0.000001) #factor=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fbf307aa-531c-49ac-87f3-b48da24513e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "Epoch: 001, Train MAE: 410.7981, Test MAE: 219.1192\n",
      "Epoch: 002, Train MAE: 214.0732, Test MAE: 246.6716\n",
      "Epoch: 003, Train MAE: 201.5073, Test MAE: 153.3506\n",
      "Epoch: 004, Train MAE: 198.7255, Test MAE: 153.3164\n",
      "Epoch: 005, Train MAE: 196.5978, Test MAE: 149.0511\n",
      "Epoch: 006, Train MAE: 189.0910, Test MAE: 151.6974\n",
      "Epoch: 007, Train MAE: 186.3607, Test MAE: 153.8979\n",
      "Epoch: 008, Train MAE: 187.8790, Test MAE: 181.6114\n",
      "Epoch: 009, Train MAE: 182.3035, Test MAE: 260.3861\n",
      "Epoch: 010, Train MAE: 184.4755, Test MAE: 133.9213\n",
      "Epoch: 011, Train MAE: 178.2305, Test MAE: 148.6062\n",
      "Epoch: 012, Train MAE: 177.3863, Test MAE: 192.8892\n",
      "Epoch: 013, Train MAE: 175.9689, Test MAE: 161.7300\n",
      "Epoch: 014, Train MAE: 175.1897, Test MAE: 213.8981\n",
      "Epoch: 015, Train MAE: 172.7912, Test MAE: 122.3280\n",
      "Epoch: 016, Train MAE: 169.7863, Test MAE: 148.4333\n",
      "Epoch: 017, Train MAE: 171.1978, Test MAE: 134.8363\n",
      "Epoch: 018, Train MAE: 167.9668, Test MAE: 120.6891\n",
      "Epoch: 019, Train MAE: 169.2919, Test MAE: 134.2033\n",
      "Epoch: 020, Train MAE: 167.8218, Test MAE: 147.1778\n",
      "Epoch: 021, Train MAE: 167.6530, Test MAE: 146.4086\n",
      "Epoch: 022, Train MAE: 167.0814, Test MAE: 225.4151\n",
      "Epoch: 023, Train MAE: 165.5567, Test MAE: 121.2577\n",
      "Epoch: 024, Train MAE: 164.5462, Test MAE: 128.7213\n",
      "Epoch: 025, Train MAE: 162.7481, Test MAE: 143.4305\n",
      "Epoch: 026, Train MAE: 163.9547, Test MAE: 140.8141\n",
      "Epoch: 027, Train MAE: 161.2476, Test MAE: 141.8792\n",
      "Epoch: 028, Train MAE: 161.4999, Test MAE: 149.7319\n",
      "Epoch: 029, Train MAE: 160.7115, Test MAE: 126.4723\n",
      "Epoch: 030, Train MAE: 162.1322, Test MAE: 160.1005\n",
      "Epoch: 031, Train MAE: 158.9391, Test MAE: 120.2291\n",
      "Epoch: 032, Train MAE: 159.1499, Test MAE: 138.9432\n",
      "Epoch: 033, Train MAE: 159.9113, Test MAE: 130.1778\n",
      "Epoch: 034, Train MAE: 158.5210, Test MAE: 127.5403\n",
      "Epoch: 035, Train MAE: 158.7066, Test MAE: 141.5534\n",
      "Epoch: 036, Train MAE: 157.2679, Test MAE: 135.9344\n",
      "Epoch: 037, Train MAE: 156.8723, Test MAE: 123.8514\n",
      "Epoch: 038, Train MAE: 156.6437, Test MAE: 139.3906\n",
      "Epoch: 039, Train MAE: 157.5710, Test MAE: 131.2189\n",
      "Epoch: 040, Train MAE: 155.5031, Test MAE: 157.5353\n",
      "Epoch: 041, Train MAE: 155.3346, Test MAE: 159.7034\n",
      "Epoch: 042, Train MAE: 154.9907, Test MAE: 155.8636\n",
      "Epoch: 043, Train MAE: 154.7624, Test MAE: 126.7731\n",
      "Epoch: 044, Train MAE: 153.7381, Test MAE: 134.6741\n",
      "Epoch: 045, Train MAE: 153.1895, Test MAE: 146.1709\n",
      "Epoch: 046, Train MAE: 152.8926, Test MAE: 129.4952\n",
      "Epoch: 047, Train MAE: 153.9962, Test MAE: 132.0968\n",
      "Epoch: 048, Train MAE: 153.0966, Test MAE: 135.2002\n",
      "Epoch: 049, Train MAE: 152.8603, Test MAE: 148.8295\n",
      "Epoch: 050, Train MAE: 150.6695, Test MAE: 130.8397\n",
      "Epoch: 051, Train MAE: 152.3497, Test MAE: 150.9531\n",
      "Epoch: 052, Train MAE: 151.4472, Test MAE: 127.3292\n",
      "Epoch: 053, Train MAE: 151.9125, Test MAE: 145.4884\n",
      "Epoch: 054, Train MAE: 150.8230, Test MAE: 125.2166\n",
      "Epoch: 055, Train MAE: 151.1172, Test MAE: 130.3196\n",
      "Epoch: 056, Train MAE: 150.9757, Test MAE: 140.2637\n",
      "Epoch: 057, Train MAE: 149.9777, Test MAE: 169.4866\n",
      "Epoch: 058, Train MAE: 149.9850, Test MAE: 147.1654\n",
      "Epoch: 059, Train MAE: 149.6541, Test MAE: 154.5661\n",
      "Epoch: 060, Train MAE: 149.0336, Test MAE: 142.4460\n",
      "Epoch: 061, Train MAE: 149.0162, Test MAE: 166.2657\n",
      "Epoch: 062, Train MAE: 148.1255, Test MAE: 174.8222\n",
      "Epoch: 063, Train MAE: 148.7070, Test MAE: 152.1501\n",
      "Epoch: 064, Train MAE: 148.7157, Test MAE: 153.2341\n",
      "Epoch: 065, Train MAE: 148.7308, Test MAE: 159.3998\n",
      "Epoch: 066, Train MAE: 148.9622, Test MAE: 152.2428\n",
      "Epoch: 067, Train MAE: 148.4795, Test MAE: 149.9031\n",
      "Epoch: 068, Train MAE: 147.7609, Test MAE: 153.1058\n",
      "Epoch: 069, Train MAE: 148.2870, Test MAE: 170.4795\n",
      "Epoch: 070, Train MAE: 147.5963, Test MAE: 157.7116\n",
      "Epoch: 071, Train MAE: 146.3859, Test MAE: 167.3605\n",
      "Epoch: 072, Train MAE: 147.7812, Test MAE: 142.5414\n",
      "Epoch: 073, Train MAE: 147.8795, Test MAE: 153.2933\n",
      "Epoch: 074, Train MAE: 147.2373, Test MAE: 166.3189\n",
      "Epoch: 075, Train MAE: 147.5632, Test MAE: 150.0514\n",
      "Epoch: 076, Train MAE: 147.2419, Test MAE: 154.5063\n",
      "Epoch: 077, Train MAE: 147.4638, Test MAE: 158.8034\n",
      "Epoch: 078, Train MAE: 147.4206, Test MAE: 147.8311\n",
      "Epoch: 079, Train MAE: 146.6212, Test MAE: 164.7973\n",
      "Epoch: 080, Train MAE: 146.3457, Test MAE: 149.9713\n",
      "Epoch: 081, Train MAE: 146.4079, Test MAE: 153.5141\n",
      "Epoch: 082, Train MAE: 146.5480, Test MAE: 142.8513\n",
      "Epoch: 083, Train MAE: 146.1521, Test MAE: 157.0171\n",
      "Epoch: 084, Train MAE: 145.9379, Test MAE: 148.9860\n",
      "Epoch: 085, Train MAE: 145.6077, Test MAE: 158.6453\n",
      "Epoch: 086, Train MAE: 145.8678, Test MAE: 154.0536\n",
      "Epoch: 087, Train MAE: 145.4080, Test MAE: 144.4288\n",
      "Epoch: 088, Train MAE: 145.2534, Test MAE: 151.5273\n",
      "Epoch: 089, Train MAE: 145.3436, Test MAE: 142.2147\n",
      "Epoch: 090, Train MAE: 145.3520, Test MAE: 147.7922\n",
      "Epoch: 091, Train MAE: 145.7201, Test MAE: 146.4119\n",
      "Epoch: 092, Train MAE: 144.7105, Test MAE: 166.2552\n",
      "Epoch: 093, Train MAE: 144.8226, Test MAE: 147.0458\n",
      "Epoch: 094, Train MAE: 145.4433, Test MAE: 146.3105\n",
      "Epoch: 095, Train MAE: 145.7035, Test MAE: 154.3051\n",
      "Epoch: 096, Train MAE: 145.6423, Test MAE: 152.5232\n",
      "Epoch: 097, Train MAE: 144.9830, Test MAE: 146.7264\n",
      "Epoch: 098, Train MAE: 145.4014, Test MAE: 155.1734\n",
      "Epoch: 099, Train MAE: 145.1131, Test MAE: 153.0379\n",
      "Epoch: 100, Train MAE: 145.3247, Test MAE: 157.9177\n",
      "Epoch: 101, Train MAE: 144.8094, Test MAE: 158.9787\n",
      "Epoch: 102, Train MAE: 144.1880, Test MAE: 156.6735\n",
      "Epoch: 103, Train MAE: 145.1829, Test MAE: 149.7504\n",
      "Epoch: 104, Train MAE: 144.3491, Test MAE: 164.6430\n",
      "Epoch: 105, Train MAE: 144.6580, Test MAE: 159.3358\n",
      "Epoch: 106, Train MAE: 144.1379, Test MAE: 162.5519\n",
      "Epoch: 107, Train MAE: 144.0273, Test MAE: 152.1993\n",
      "Epoch: 108, Train MAE: 144.8419, Test MAE: 153.4528\n",
      "Epoch: 109, Train MAE: 144.8246, Test MAE: 147.9005\n",
      "Epoch: 110, Train MAE: 143.9261, Test MAE: 155.7733\n",
      "Epoch: 111, Train MAE: 144.4356, Test MAE: 158.0373\n",
      "Epoch: 112, Train MAE: 144.2364, Test MAE: 158.2883\n",
      "Epoch: 113, Train MAE: 143.9556, Test MAE: 148.7334\n",
      "Epoch: 114, Train MAE: 144.3426, Test MAE: 158.4650\n",
      "Epoch: 115, Train MAE: 144.3876, Test MAE: 168.1816\n",
      "Epoch: 116, Train MAE: 144.0985, Test MAE: 151.3764\n",
      "Epoch: 117, Train MAE: 143.6768, Test MAE: 154.7800\n",
      "Epoch: 118, Train MAE: 143.9685, Test MAE: 157.4150\n",
      "Epoch: 119, Train MAE: 144.6008, Test MAE: 164.3045\n",
      "Epoch: 120, Train MAE: 145.0017, Test MAE: 157.9740\n",
      "Epoch: 121, Train MAE: 144.9858, Test MAE: 152.9056\n",
      "Epoch: 122, Train MAE: 143.5935, Test MAE: 156.8144\n",
      "Epoch: 123, Train MAE: 143.8841, Test MAE: 166.9635\n",
      "Epoch: 124, Train MAE: 143.9526, Test MAE: 152.6844\n",
      "Epoch: 125, Train MAE: 144.1590, Test MAE: 156.0894\n",
      "Epoch: 126, Train MAE: 144.0406, Test MAE: 153.8753\n",
      "Epoch: 127, Train MAE: 143.8526, Test MAE: 153.2153\n",
      "Epoch: 128, Train MAE: 144.3413, Test MAE: 165.0059\n",
      "Epoch: 129, Train MAE: 144.4282, Test MAE: 155.1000\n",
      "Epoch: 130, Train MAE: 143.6360, Test MAE: 153.2800\n",
      "Epoch: 131, Train MAE: 143.3164, Test MAE: 154.5853\n",
      "Epoch: 132, Train MAE: 143.9723, Test MAE: 156.3524\n",
      "Epoch: 133, Train MAE: 144.0091, Test MAE: 159.1161\n",
      "Epoch: 134, Train MAE: 143.5182, Test MAE: 158.2697\n",
      "Epoch: 135, Train MAE: 144.0905, Test MAE: 162.6144\n",
      "Epoch: 136, Train MAE: 143.9265, Test MAE: 161.5950\n",
      "Epoch: 137, Train MAE: 143.1957, Test MAE: 159.4653\n",
      "Epoch: 138, Train MAE: 144.0541, Test MAE: 158.0539\n",
      "Epoch: 139, Train MAE: 142.9004, Test MAE: 155.7365\n",
      "Epoch: 140, Train MAE: 143.8830, Test MAE: 156.1181\n",
      "Epoch: 141, Train MAE: 143.9094, Test MAE: 153.3385\n",
      "Epoch: 142, Train MAE: 143.6276, Test MAE: 155.4910\n",
      "Epoch: 143, Train MAE: 143.7674, Test MAE: 160.6836\n",
      "Epoch: 144, Train MAE: 143.6403, Test MAE: 154.0558\n",
      "Epoch: 145, Train MAE: 143.9825, Test MAE: 156.0781\n",
      "Epoch: 146, Train MAE: 143.8794, Test MAE: 158.9383\n",
      "Epoch: 147, Train MAE: 143.6998, Test MAE: 155.6494\n",
      "Epoch: 148, Train MAE: 144.4058, Test MAE: 158.7973\n",
      "Epoch: 149, Train MAE: 143.8049, Test MAE: 158.7857\n",
      "Epoch: 150, Train MAE: 143.7047, Test MAE: 155.0187\n",
      "Epoch: 151, Train MAE: 143.4854, Test MAE: 156.4613\n",
      "Epoch: 152, Train MAE: 143.5832, Test MAE: 154.3262\n",
      "Epoch: 153, Train MAE: 144.1448, Test MAE: 157.8387\n",
      "Epoch: 154, Train MAE: 144.2256, Test MAE: 156.1670\n",
      "Epoch: 155, Train MAE: 144.1403, Test MAE: 159.4780\n",
      "Epoch: 156, Train MAE: 144.3084, Test MAE: 160.6824\n",
      "Epoch: 157, Train MAE: 143.6589, Test MAE: 158.1523\n",
      "Epoch: 158, Train MAE: 143.1077, Test MAE: 158.7042\n",
      "Epoch: 159, Train MAE: 143.3121, Test MAE: 157.2799\n",
      "Epoch: 160, Train MAE: 143.6987, Test MAE: 156.7065\n",
      "Epoch: 161, Train MAE: 143.8422, Test MAE: 158.5924\n",
      "Epoch: 162, Train MAE: 143.6781, Test MAE: 157.9495\n",
      "Epoch: 163, Train MAE: 144.4523, Test MAE: 158.1351\n",
      "Epoch: 164, Train MAE: 143.3650, Test MAE: 155.6835\n",
      "Epoch: 165, Train MAE: 143.5490, Test MAE: 157.3059\n",
      "Epoch: 166, Train MAE: 143.6972, Test MAE: 156.6096\n",
      "Epoch: 167, Train MAE: 143.2985, Test MAE: 159.5480\n",
      "Epoch: 168, Train MAE: 143.5852, Test MAE: 157.5652\n",
      "Epoch: 169, Train MAE: 143.0734, Test MAE: 156.3332\n",
      "Epoch: 170, Train MAE: 144.2888, Test MAE: 156.9156\n",
      "Epoch: 171, Train MAE: 143.3007, Test MAE: 156.0175\n",
      "Epoch: 172, Train MAE: 142.8131, Test MAE: 158.3016\n",
      "Epoch: 173, Train MAE: 143.6157, Test MAE: 156.9749\n",
      "Epoch: 174, Train MAE: 143.3549, Test MAE: 157.3900\n",
      "Epoch: 175, Train MAE: 143.9365, Test MAE: 159.1795\n",
      "Epoch: 176, Train MAE: 144.4165, Test MAE: 159.1308\n",
      "Epoch: 177, Train MAE: 143.3211, Test MAE: 156.6836\n",
      "Epoch: 178, Train MAE: 144.0626, Test MAE: 156.6316\n",
      "Epoch: 179, Train MAE: 143.3956, Test MAE: 157.5239\n",
      "Epoch: 180, Train MAE: 144.0420, Test MAE: 156.7094\n",
      "Epoch: 181, Train MAE: 143.7358, Test MAE: 158.2416\n",
      "Epoch: 182, Train MAE: 144.1389, Test MAE: 156.2960\n",
      "Epoch: 183, Train MAE: 143.8957, Test MAE: 157.7220\n",
      "Epoch: 184, Train MAE: 143.8080, Test MAE: 157.9146\n",
      "Epoch: 185, Train MAE: 143.4353, Test MAE: 157.0189\n",
      "Epoch: 186, Train MAE: 143.5970, Test MAE: 157.2600\n",
      "Epoch: 187, Train MAE: 143.5851, Test MAE: 158.1620\n",
      "Epoch: 188, Train MAE: 143.3745, Test MAE: 158.1523\n",
      "Epoch: 189, Train MAE: 143.1372, Test MAE: 158.6592\n",
      "Epoch: 190, Train MAE: 143.4346, Test MAE: 155.4302\n",
      "Epoch: 191, Train MAE: 143.5371, Test MAE: 157.3201\n",
      "Epoch: 192, Train MAE: 144.2718, Test MAE: 158.9887\n",
      "Epoch: 193, Train MAE: 143.6932, Test MAE: 156.3352\n",
      "Epoch: 194, Train MAE: 143.8211, Test MAE: 159.2113\n",
      "Epoch: 195, Train MAE: 143.1952, Test MAE: 156.8871\n",
      "Epoch: 196, Train MAE: 143.2621, Test MAE: 156.5585\n",
      "Epoch: 197, Train MAE: 142.4983, Test MAE: 156.9791\n",
      "Epoch: 198, Train MAE: 144.4542, Test MAE: 156.4338\n",
      "Epoch: 199, Train MAE: 144.0957, Test MAE: 158.0809\n",
      "Epoch: 200, Train MAE: 143.7698, Test MAE: 156.5152\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 200\n",
    "print('start train')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_acc = train(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    train_loss.append(train_acc)\n",
    "    test_loss.append(test_acc)\n",
    "    scheduler.step(test_acc)\n",
    "    print(f'Epoch: {epoch+1:03d}, Train MAE: {train_acc:.4f}, Test MAE: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35131580-625c-463b-ae2d-9d78aec742df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(np.arange(epochs), train_loss, label='train loss')\n",
    "plt.plot(np.arange(epochs), test_loss, label='val loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b497b990-bad4-4a60-a567-4fb855089f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    torch.save(model.state_dict(), \"model/best_numGNN2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea24b96-0bb4-4825-8511-025d23c07178",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2b58b-befe-46af-9ac2-cb889717fceb",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e876ea1-3a3e-4f5d-bfa6-420b19b1c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 65625\n",
      "Number of test graphs: 16237\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pickle.load(open('data/train/graph_concat.pkl', 'rb'))\n",
    "test_dataset = pickle.load(open('data/test/graph_concat.pkl', 'rb'))\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b15ad756-929a-4341-a850-2443109b32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = []\n",
    "test_feat = []\n",
    "\n",
    "for train_data in train_dataset:\n",
    "    train_feat.append(train_data.x_feat)\n",
    "    \n",
    "for test_data in test_dataset:\n",
    "    test_feat.append(test_data.x_feat)\n",
    "\n",
    "\n",
    "train_feat = torch.cat(train_feat, axis=0)\n",
    "test_feat = torch.cat(test_feat, axis=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_feat = torch.from_numpy(sc.fit_transform(train_feat))\n",
    "test_feat = torch.from_numpy(sc.transform(test_feat))\n",
    "\n",
    "for i, data in enumerate(train_dataset):\n",
    "    data.x_feat = train_feat[i].unsqueeze(0)\n",
    "\n",
    "for i, data in enumerate(test_dataset):\n",
    "    data.x_feat = test_feat[i].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8c5717e-5674-4121-966a-c3babae5999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea2ece4-05e7-469a-a880-0eb27fe99531",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "160f8d28-db63-4202-ab0f-64e030d9b4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (g): SAGE(\n",
       "    (conv1): SAGEConv(9, 32)\n",
       "    (conv2): SAGEConv(32, 32)\n",
       "    (conv3): SAGEConv(32, 32)\n",
       "    (conv4): SAGEConv(32, 32)\n",
       "    (lin): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (lin1): Linear(in_features=21, out_features=32, bias=True)\n",
       "    (lin2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin4): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (lin): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (cross): CrossNet()\n",
       "  (mlp_cross): MLP(\n",
       "    (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp_out): MLP(\n",
       "    (lin1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_node_features = 9\n",
    "hidden_channels = 32\n",
    "num_feats = 21\n",
    "num_classes = 1\n",
    "num_attr = 3\n",
    "\n",
    "model = Net(num_node_features, num_feats, hidden_channels, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #0.001\n",
    "criterion = torch.nn.L1Loss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.85, patience=3, min_lr=0.000001)\n",
    "\n",
    "model.load_state_dict(torch.load('model/best_SAGE_concat.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f3c94-3bb9-44c0-acc4-8e582aeac2d3",
   "metadata": {},
   "source": [
    "## Evaluate Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35b0b9ff-5788-47ec-a798-27ac83cac519",
   "metadata": {},
   "outputs": [],
   "source": [
    "mofname = []\n",
    "co2_select = []\n",
    "\n",
    "for data in test_loader:  # Iterate in batches over the training dataset.\n",
    "    data.to(device)\n",
    "    \n",
    "    out = model(data.x, data.edge_index, data.batch, data.x_feat.float()).cpu().detach().numpy()\n",
    "    mofname.append(data.mofname)\n",
    "    co2_select.append(out)\n",
    "    \n",
    "mofname = np.concatenate(mofname)\n",
    "co2_select = np.concatenate(co2_select).flatten()\n",
    "\n",
    "cut_mof_unit = lambda x: x.split('_')[-1]\n",
    "id_ = np.array(list(map(cut_mof_unit, mofname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9db2345b-cf9e-43c3-bc68-67993c405a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id': id_, 'CO2_working_capacity [mL/g]': co2_select}\n",
    "\n",
    "df_inference = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7435fc60-3757-427b-a8c1-bffe25e4fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgboost = pd.read_csv('xgboost_submission.csv')\n",
    "df_xgboost = df_xgboost.set_index('id')\n",
    "\n",
    "df_xgboost.loc[df_inference.id.values.astype(int)] = np.expand_dims(df_inference['CO2_working_capacity [mL/g]'].values, axis=1)\n",
    "df_xgboost = df_xgboost.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "efb0471b-287b-4ffa-b888-f363e344f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgboost.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1fb8d3ae-2411-4ea2-856f-fe7cd2c5d9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>CO2_working_capacity [mL/g]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68614</td>\n",
       "      <td>173.341354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68615</td>\n",
       "      <td>67.073875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68616</td>\n",
       "      <td>60.989979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68617</td>\n",
       "      <td>56.514645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68618</td>\n",
       "      <td>63.594818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16995</th>\n",
       "      <td>85609</td>\n",
       "      <td>-7.164486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>85610</td>\n",
       "      <td>1.655503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16997</th>\n",
       "      <td>85611</td>\n",
       "      <td>-0.521235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16998</th>\n",
       "      <td>85612</td>\n",
       "      <td>-1.570179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999</th>\n",
       "      <td>85613</td>\n",
       "      <td>-5.174440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  CO2_working_capacity [mL/g]\n",
       "0      68614                   173.341354\n",
       "1      68615                    67.073875\n",
       "2      68616                    60.989979\n",
       "3      68617                    56.514645\n",
       "4      68618                    63.594818\n",
       "...      ...                          ...\n",
       "16995  85609                    -7.164486\n",
       "16996  85610                     1.655503\n",
       "16997  85611                    -0.521235\n",
       "16998  85612                    -1.570179\n",
       "16999  85613                    -5.174440\n",
       "\n",
       "[17000 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccfd22f-5ebe-43cd-9621-06a0ddcd6f37",
   "metadata": {},
   "source": [
    "## Create Latent Space for AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3da5e7a7-88cc-46b5-a7e5-2aca58e7215b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (g): SAGE(\n",
       "    (conv1): SAGEConv(9, 32)\n",
       "    (conv2): SAGEConv(32, 32)\n",
       "    (conv3): SAGEConv(32, 32)\n",
       "    (conv4): SAGEConv(32, 32)\n",
       "    (lin): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (lin1): Linear(in_features=21, out_features=32, bias=True)\n",
       "    (lin2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin4): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (lin): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (cross): CrossNet()\n",
       "  (mlp_cross): MLP(\n",
       "    (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp_out): MLP(\n",
       "    (lin1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (out): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.out=nn.Sequential(*list(model.out.children())[:-1])\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91628a15-bd6f-4df6-812e-d86634bc8e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 100\n",
      "done: 200\n",
      "done: 300\n",
      "done: 400\n",
      "done: 500\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "    data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch, data.x_feat.float()).cpu().detach().numpy()\n",
    "    x_feat = data.x_feat.cpu().detach().numpy()\n",
    "\n",
    "    out = np.concatenate((out, x_feat), axis=1)\n",
    "    train_x.append(out)\n",
    "    train_y.append(data.y.cpu().detach().numpy())\n",
    "    c=c+1\n",
    "    if(c%100==0):\n",
    "        print('done:',c)\n",
    "\n",
    "train_x = np.concatenate(train_x, axis=0)\n",
    "train_y = np.concatenate(train_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00dc8e64-9c19-401b-864d-13b4539fa5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 100\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "test_x = []\n",
    "test_mofname = []\n",
    "\n",
    "for data in test_loader:  # Iterate in batches over the training dataset.\n",
    "    data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch, data.x_feat.float()).cpu().detach().numpy()\n",
    "    x_feat = data.x_feat.cpu().detach().numpy()\n",
    "\n",
    "    out = np.concatenate((out, x_feat), axis=1)\n",
    "    test_x.append(out)\n",
    "    test_mofname.append(data.mofname)\n",
    "    c=c+1\n",
    "    if(c%100==0):\n",
    "        print('done:',c)\n",
    "\n",
    "test_x = np.concatenate(test_x, axis=0)\n",
    "test_mofname = np.concatenate(test_mofname, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8b951cd-46fd-4d9a-b530-cdf6e6cfc44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_x)\n",
    "test_df = pd.DataFrame(test_x)\n",
    "\n",
    "train_df['target'] = train_y.flatten()\n",
    "test_df['mofname'] = test_mofname.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cf83f50-e2a7-4ab8-8c32-aec32212f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>mofname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.393690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.533949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.920761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.790710</td>\n",
       "      <td>-0.882568</td>\n",
       "      <td>-0.571121</td>\n",
       "      <td>-0.711980</td>\n",
       "      <td>-0.610755</td>\n",
       "      <td>0.547042</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>0.050804</td>\n",
       "      <td>0.731336</td>\n",
       "      <td>mof_unit_68614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.179619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.036682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.112279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.596504</td>\n",
       "      <td>-0.279894</td>\n",
       "      <td>-0.491427</td>\n",
       "      <td>-0.082455</td>\n",
       "      <td>-0.518308</td>\n",
       "      <td>-0.334211</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.067150</td>\n",
       "      <td>-0.585683</td>\n",
       "      <td>mof_unit_68615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.570940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.469352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.490082</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085405</td>\n",
       "      <td>0.476304</td>\n",
       "      <td>0.123035</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.056075</td>\n",
       "      <td>-0.334211</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.101586</td>\n",
       "      <td>-0.637351</td>\n",
       "      <td>mof_unit_68616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.089923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.092972</td>\n",
       "      <td>...</td>\n",
       "      <td>1.352204</td>\n",
       "      <td>0.873918</td>\n",
       "      <td>0.453466</td>\n",
       "      <td>-0.711980</td>\n",
       "      <td>-0.980542</td>\n",
       "      <td>0.742876</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.123961</td>\n",
       "      <td>-0.594873</td>\n",
       "      <td>mof_unit_68617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.540619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.358086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.436696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736885</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.352755</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.333415</td>\n",
       "      <td>-0.138377</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.102154</td>\n",
       "      <td>-0.468214</td>\n",
       "      <td>mof_unit_68618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16232</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.088397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.394831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.958029</td>\n",
       "      <td>3.118466</td>\n",
       "      <td>5.937423</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.795648</td>\n",
       "      <td>-0.627962</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.156696</td>\n",
       "      <td>-1.963071</td>\n",
       "      <td>mof_unit_85609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16233</th>\n",
       "      <td>0.313762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.443608</td>\n",
       "      <td>0.289820</td>\n",
       "      <td>0.350841</td>\n",
       "      <td>0.267429</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714540</td>\n",
       "      <td>2.005436</td>\n",
       "      <td>1.721716</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.980542</td>\n",
       "      <td>-0.921714</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.142419</td>\n",
       "      <td>-1.605460</td>\n",
       "      <td>mof_unit_85610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16234</th>\n",
       "      <td>0.248444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590385</td>\n",
       "      <td>0.052065</td>\n",
       "      <td>0.573007</td>\n",
       "      <td>0.023652</td>\n",
       "      <td>...</td>\n",
       "      <td>1.912784</td>\n",
       "      <td>1.749141</td>\n",
       "      <td>1.644934</td>\n",
       "      <td>-0.711980</td>\n",
       "      <td>-0.425862</td>\n",
       "      <td>0.253291</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.155858</td>\n",
       "      <td>-1.651416</td>\n",
       "      <td>mof_unit_85611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16235</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.856547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.944257</td>\n",
       "      <td>2.093103</td>\n",
       "      <td>2.144743</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.425862</td>\n",
       "      <td>0.449125</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.154843</td>\n",
       "      <td>-1.748159</td>\n",
       "      <td>mof_unit_85612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16236</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.708716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.926558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.436823</td>\n",
       "      <td>2.762963</td>\n",
       "      <td>4.032448</td>\n",
       "      <td>-1.026742</td>\n",
       "      <td>-0.888095</td>\n",
       "      <td>0.644959</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.158366</td>\n",
       "      <td>-1.946889</td>\n",
       "      <td>mof_unit_85613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16237 rows × 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0    1    2    3    4    5         6          7         8  \\\n",
       "0      28.393690  0.0  0.0  0.0  0.0  0.0  0.000000  30.533949  0.000000   \n",
       "1      10.179619  0.0  0.0  0.0  0.0  0.0  0.000000  11.036682  0.000000   \n",
       "2       9.570940  0.0  0.0  0.0  0.0  0.0  0.000000  10.469352  0.000000   \n",
       "3       8.316006  0.0  0.0  0.0  0.0  0.0  0.000000   9.089923  0.000000   \n",
       "4       9.540619  0.0  0.0  0.0  0.0  0.0  0.000000  10.358086  0.000000   \n",
       "...          ...  ...  ...  ...  ...  ...       ...        ...       ...   \n",
       "16232   0.000000  0.0  0.0  0.0  0.0  0.0  2.088397   0.000000  2.394831   \n",
       "16233   0.313762  0.0  0.0  0.0  0.0  0.0  0.443608   0.289820  0.350841   \n",
       "16234   0.248444  0.0  0.0  0.0  0.0  0.0  0.590385   0.052065  0.573007   \n",
       "16235   0.000000  0.0  0.0  0.0  0.0  0.0  0.856547   0.000000  0.859740   \n",
       "16236   0.000000  0.0  0.0  0.0  0.0  0.0  1.708716   0.000000  1.926558   \n",
       "\n",
       "               9  ...       268       269       270       271       272  \\\n",
       "0      30.920761  ... -0.790710 -0.882568 -0.571121 -0.711980 -0.610755   \n",
       "1      11.112279  ... -0.596504 -0.279894 -0.491427 -0.082455 -0.518308   \n",
       "2      10.490082  ...  1.085405  0.476304  0.123035 -0.397217 -0.056075   \n",
       "3       9.092972  ...  1.352204  0.873918  0.453466 -0.711980 -0.980542   \n",
       "4      10.436696  ...  0.736885  0.782988  0.352755 -0.397217 -0.333415   \n",
       "...          ...  ...       ...       ...       ...       ...       ...   \n",
       "16232   0.000000  ...  2.958029  3.118466  5.937423 -0.397217 -0.795648   \n",
       "16233   0.267429  ...  1.714540  2.005436  1.721716 -0.397217 -0.980542   \n",
       "16234   0.023652  ...  1.912784  1.749141  1.644934 -0.711980 -0.425862   \n",
       "16235   0.000000  ...  1.944257  2.093103  2.144743 -0.397217 -0.425862   \n",
       "16236   0.000000  ...  2.436823  2.762963  4.032448 -1.026742 -0.888095   \n",
       "\n",
       "            273       274       275       276         mofname  \n",
       "0      0.547042  0.359778  0.050804  0.731336  mof_unit_68614  \n",
       "1     -0.334211  0.359778 -0.067150 -0.585683  mof_unit_68615  \n",
       "2     -0.334211  0.359778 -0.101586 -0.637351  mof_unit_68616  \n",
       "3      0.742876  0.359778 -0.123961 -0.594873  mof_unit_68617  \n",
       "4     -0.138377  0.359778 -0.102154 -0.468214  mof_unit_68618  \n",
       "...         ...       ...       ...       ...             ...  \n",
       "16232 -0.627962 -2.779491 -0.156696 -1.963071  mof_unit_85609  \n",
       "16233 -0.921714 -2.779491 -0.142419 -1.605460  mof_unit_85610  \n",
       "16234  0.253291 -2.779491 -0.155858 -1.651416  mof_unit_85611  \n",
       "16235  0.449125 -2.779491 -0.154843 -1.748159  mof_unit_85612  \n",
       "16236  0.644959 -2.779491 -0.158366 -1.946889  mof_unit_85613  \n",
       "\n",
       "[16237 rows x 278 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c59a166-f0e8-438e-95fb-49c5fa3af489",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/train/auto_SAGE_feat_train.csv',index=False)\n",
    "test_df.to_csv('data/test/auto_SAGE_feat_mofname_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12171eea-b02a-49f7-b95d-493d132fec69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMLCC_CUDA",
   "language": "python",
   "name": "tmlcc_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
