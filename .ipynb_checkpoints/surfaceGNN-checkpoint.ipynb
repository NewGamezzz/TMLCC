{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08d8716-5267-467e-8fbf-f956bfbeeaca",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38601030-b6e2-4ea2-9844-49c943334f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GINConv, GINEConv, GCNConv, GraphConv, SAGEConv, ChebConv, global_add_pool, global_mean_pool\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b376f8b-82c5-40bb-ba48-0dedd85f7591",
   "metadata": {},
   "source": [
    "# Run Pytorch on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f424459d-d858-4255-9250-1f940079f474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_pytorch_version(version):\n",
    "    return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "    return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34ea2e-7a10-4e27-8fe3-29fb7b0132a6",
   "metadata": {},
   "source": [
    "# DataSet & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac311ce8-9e84-4c67-8d4e-2d8f8659cbf2",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6066e87-d49c-4207-8c46-cb7983177cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topo_0                                           0\n",
      "topo_1                                           0\n",
      "topo_2                                           0\n",
      "topo_3                                           0\n",
      "topo_4                                           0\n",
      "topo_5                                           0\n",
      "topo_6                                           0\n",
      "topo_7                                           0\n",
      "topo_8                                           0\n",
      "topo_9                                           0\n",
      "MOFname                                          0\n",
      "volume [A^3]                                     0\n",
      "weight [u]                                       0\n",
      "density [g/cm^3]                                 0\n",
      "surface_area [m^2/g]                             0\n",
      "void_fraction                                    0\n",
      "void_volume [cm^3/g]                             0\n",
      "functional_groups                                0\n",
      "metal_linker                                     0\n",
      "organic_linker1                                  0\n",
      "organic_linker2                                  0\n",
      "catalog CO2/N2                                   0\n",
      "CO2/N2_selectivity                               0\n",
      "heat_adsorption_CO2_P0.15bar_T298K [kcal/mol]    0\n",
      "Smiles                                           0\n",
      "dtype: int64\n",
      "(17000, 25)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/test/clean_test.csv')\n",
    "smiles = pd.read_csv('data/test/smiles_test.csv')\n",
    "data = df.join(smiles.set_index('MOFname'), on='MOFname')\n",
    "\n",
    "data = data.dropna(subset=['Smiles'])\n",
    "data = data.reset_index(drop=True)\n",
    "print(data.isnull().sum())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11f0e34-b17a-4f43-9a1a-0b4f0d2fc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map = {\n",
    "    'atomic_num':\n",
    "    list(range(0, 119)),\n",
    "    'chirality': [\n",
    "        'CHI_UNSPECIFIED',\n",
    "        'CHI_TETRAHEDRAL_CW',\n",
    "        'CHI_TETRAHEDRAL_CCW',\n",
    "        'CHI_OTHER',\n",
    "    ],\n",
    "    'degree':\n",
    "    list(range(0, 11)),\n",
    "    'formal_charge':\n",
    "    list(range(-5, 7)),\n",
    "    'num_hs':\n",
    "    list(range(0, 9)),\n",
    "    'num_radical_electrons':\n",
    "    list(range(0, 5)),\n",
    "    'hybridization': [\n",
    "        'UNSPECIFIED',\n",
    "        'S',\n",
    "        'SP',\n",
    "        'SP2',\n",
    "        'SP3',\n",
    "        'SP3D',\n",
    "        'SP3D2',\n",
    "        'OTHER',\n",
    "    ],\n",
    "    'is_aromatic': [False, True],\n",
    "    'is_in_ring': [False, True],\n",
    "}\n",
    "\n",
    "e_map = {\n",
    "    'bond_type': [\n",
    "        'misc',\n",
    "        'SINGLE',\n",
    "        'DOUBLE',\n",
    "        'TRIPLE',\n",
    "        'AROMATIC',\n",
    "    ],\n",
    "    'stereo': [\n",
    "        'STEREONONE',\n",
    "        'STEREOZ',\n",
    "        'STEREOE',\n",
    "        'STEREOCIS',\n",
    "        'STEREOTRANS',\n",
    "        'STEREOANY',\n",
    "    ],\n",
    "    'is_conjugated': [False, True],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54ebf3-b60c-40b5-97cc-2d12fb058556",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "data_dict = []\n",
    "c = 1\n",
    "for _, line in data.iterrows():\n",
    "    mol = Chem.MolFromSmiles(line['Smiles'])\n",
    "    \n",
    "    if mol == None:\n",
    "        continue\n",
    "    \n",
    "    # Create Node Features\n",
    "    xs = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        x = []\n",
    "        x.append(x_map['atomic_num'].index(atom.GetAtomicNum()))\n",
    "        x.append(x_map['chirality'].index(str(atom.GetChiralTag())))\n",
    "        x.append(x_map['degree'].index(atom.GetTotalDegree()))\n",
    "        x.append(x_map['formal_charge'].index(atom.GetFormalCharge()))\n",
    "        x.append(x_map['num_hs'].index(atom.GetTotalNumHs()))\n",
    "        x.append(x_map['num_radical_electrons'].index(atom.GetNumRadicalElectrons()))\n",
    "        x.append(x_map['hybridization'].index(str(atom.GetHybridization())))\n",
    "        x.append(x_map['is_aromatic'].index(atom.GetIsAromatic()))\n",
    "        x.append(x_map['is_in_ring'].index(atom.IsInRing()))\n",
    "        xs.append(x)\n",
    "    x = torch.tensor(xs, dtype=torch.float).view(-1, 9)\n",
    "    \n",
    "    # Create Edge Features\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        e = []\n",
    "        e.append(e_map['bond_type'].index(str(bond.GetBondType())))\n",
    "        e.append(e_map['stereo'].index(str(bond.GetStereo())))\n",
    "        e.append(e_map['is_conjugated'].index(bond.GetIsConjugated()))\n",
    "\n",
    "        edge_indices += [[i, j], [j, i]]\n",
    "        edge_attrs += [e, e]\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices)\n",
    "    edge_index = edge_index.t().to(torch.long).view(2, -1)\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3)\n",
    "\n",
    "    # Sort indices.\n",
    "    if edge_index.numel() > 0:\n",
    "        perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()\n",
    "        edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]\n",
    "\n",
    "    x_feat = line.drop(['MOFname', 'weight [u]', 'functional_groups', 'CO2_working_capacity [mL/g]', 'Smiles']).values.astype(float) #, 'weight [u]', 'functional_groups', 'CO2_working_capacity [mL/g]'\n",
    "    x_feat = np.expand_dims(x_feat, axis=0)\n",
    "    x_feat = torch.tensor(x_feat)\n",
    "    y=torch.tensor([line['CO2_working_capacity [mL/g]']], dtype=torch.float).view(1, -1)\n",
    "    data_d = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, smiles=line['Smiles'], mofname=line['MOFname'], x_feat=x_feat) #, y=y\n",
    "    data_d.num_nodes = len(mol.GetAtoms())\n",
    "    data_list.append(data_d)\n",
    "    data_dict.append(line['MOFname'])\n",
    "    \n",
    "    if(c%10000==0):\n",
    "        print('done:',c)\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94bd46-dbbc-4b35-8136-4cbdf8b6630d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aa0214f3-676a-4117-a494-237cab095356",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = pickle.load(open('data/train/graph_concat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d9e88aa4-e0cf-42cb-917c-f666281c8f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 55125\n",
      "Number of test graphs: 10500\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "datasets = data_list\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(datasets, test_size=0.16, random_state = 1, shuffle=True)\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c803d206-82dc-46b1-a86a-3dfbb2364d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = []\n",
    "test_feat = []\n",
    "\n",
    "for train_data in train_dataset:\n",
    "    train_feat.append(train_data.x_feat)\n",
    "    \n",
    "for test_data in test_dataset:\n",
    "    test_feat.append(test_data.x_feat)\n",
    "\n",
    "\n",
    "train_feat = torch.cat(train_feat, axis=0)\n",
    "test_feat = torch.cat(test_feat, axis=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_feat = torch.from_numpy(sc.fit_transform(train_feat))\n",
    "test_feat = torch.from_numpy(sc.transform(test_feat))\n",
    "\n",
    "for i, data in enumerate(train_dataset):\n",
    "    data.x_feat = train_feat[i].unsqueeze(0)\n",
    "\n",
    "for i, data in enumerate(test_dataset):\n",
    "    data.x_feat = test_feat[i].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "167bf06b-c717-477f-bfcb-251a0fc2bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "data_loader = DataLoader(datasets, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4fcce-2a3b-4868-94cf-9d941c63fded",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde77d00-5597-4195-9fa5-96acd3649773",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GINE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7735e0-f581-4bd5-97a8-ca21042e8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, in_attr, dim, out_channels):\n",
    "        super(GINE, self).__init__()\n",
    "\n",
    "        self.attr1 = Sequential(Linear(in_attr, in_channels), BatchNorm1d(in_channels), ReLU())\n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(in_channels, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr2 = Sequential(Linear(in_channels, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr3 = Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr4 = Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.attr5 = Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.lin1 = Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU())\n",
    "        self.lin2 = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        edge_attr = self.attr1(edge_attr)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr2(edge_attr)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr3(edge_attr)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr4(edge_attr)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        \n",
    "        edge_attr = self.attr5(edge_attr)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "        \n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b59b767-83ce-4ca5-b20a-2fd757af246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(in_channels, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv4 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.conv5 = GINConv(\n",
    "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\n",
    "                       Linear(dim, dim), ReLU()))\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "        self.lin2 = Linear(dim, out_channels)\n",
    "        self.lin3 = Linear(out_channels, out_channels)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fafd152-2a49-4276-85de-06958a565f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, dim)\n",
    "        self.conv2 = GCNConv(dim, dim)\n",
    "        self.conv3 = GCNConv(dim, dim)\n",
    "        self.conv4 = GCNConv(dim, dim)\n",
    "        self.lin = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "789f7ab1-9725-4307-86d6-c64800f952e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, dim)\n",
    "        self.conv2 = SAGEConv(dim, dim)\n",
    "        self.conv3 = SAGEConv(dim, dim)\n",
    "        self.conv4 = SAGEConv(dim, dim)\n",
    "        self.lin = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ffba7f3-45e4-495d-a44d-8c9bd2fb90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(Graph, self).__init__()\n",
    "        self.conv1 = GraphConv(in_channels, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.conv3 = GraphConv(dim, dim)\n",
    "        self.conv4 = GraphConv(dim, dim)\n",
    "        self.lin = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377f1cd-b88e-4662-831b-4480b173f1b4",
   "metadata": {},
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a9fc53d-c55d-41a4-8083-e1aabf5fb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, dim, out_channels):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lin1 = Linear(in_channels, dim)\n",
    "        self.lin2 = Linear(dim, dim)\n",
    "        self.lin3 = Linear(dim, dim)\n",
    "        self.lin4 = Linear(dim, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x).relu()\n",
    "#         x = self.lin3(x).relu()\n",
    "        x = self.lin4(x).relu()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a19426-45fa-4120-bc5e-e46b44a7f5c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2dbed4-82bb-45f7-9cc1-ef577c1b1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, layer_num=2):\n",
    "        super(CrossNet, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.kernels = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "\n",
    "        for i in range(self.kernels.shape[0]):\n",
    "            nn.init.xavier_normal_(self.kernels[i])\n",
    "        for i in range(self.bias.shape[0]):\n",
    "            nn.init.zeros_(self.bias[i])\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_0 = inputs.unsqueeze(2)\n",
    "        x_l = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            xl_w = torch.tensordot(x_l, self.kernels[i], dims=([1], [0]))\n",
    "            dot_ = torch.matmul(x_0, xl_w)\n",
    "            x_l = dot_ + self.bias[i] + x_l\n",
    "        x_l = torch.squeeze(x_l, dim=2)\n",
    "        return x_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82eb29-3e47-45f7-afd4-3c28c6963fd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multi Input Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8e2a6a-ba42-4c4a-8de0-bf276fd3976b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, in_xs, in_attr, in_xfeats, dim, out_channels):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.gine = GINE(in_xs, in_attr, dim, 128)\n",
    "#         self.mlp_num = MLP(in_xfeats, dim, 128)\n",
    "#         self.bn = BatchNorm1d(256)\n",
    "#         self.lin = Sequential(Linear(256, 128), BatchNorm1d(128))\n",
    "#         self.lin2 = Sequential(Linear(128, 128), BatchNorm1d(128))\n",
    "#         # Deep & Cross Network\n",
    "#         self.crossnet = CrossNet(128)\n",
    "#         self.mlp_cross = MLP(128, 256, 128)\n",
    "        \n",
    "#         self.bn_cat = BatchNorm1d(256)\n",
    "#         self.mlp_cat = MLP(256, 256, 128)\n",
    "#         self.dropout = Dropout(p=0.5)\n",
    "#         self.out = Linear(128, out_channels) # 256\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_attr, batch, x_feat):\n",
    "#         x = self.gine(x, edge_index, edge_attr, batch)\n",
    "#         x_feat = self.mlp_num(x_feat)\n",
    "#         concat = torch.cat((x, x_feat),dim=1)\n",
    "#         x = self.bn(concat)\n",
    "#         x = self.lin(x)\n",
    "#         x = self.lin2(x)\n",
    "        \n",
    "#         # Deep & Cross Network\n",
    "#         hl = self.mlp_cross(x)\n",
    "#         xl = self.crossnet(x)\n",
    "#         x = torch.cat((xl, hl), dim=1)\n",
    "#         x = self.bn_cat(x)\n",
    "#         x = self.mlp_cat(x)\n",
    "#         x = self.dropout(x)\n",
    "#         out = self.out(x)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d00365a0-505e-4adf-b703-f17774e8e674",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, in_xs, in_attr, in_xfeats, dim, out_channels):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.gin = GCN(in_xs, dim, 128)\n",
    "#         self.mlp_num = MLP(in_xfeats, dim, 128)\n",
    "#         self.bn = BatchNorm1d(256)\n",
    "#         # Deep & Cross Network\n",
    "#         self.crossnet = CrossNet(256, layer_num=2)\n",
    "#         self.mlp_cross = MLP(256, 256, 256)\n",
    "        \n",
    "#         self.bn_cat = BatchNorm1d(256) #64+in_xfeats\n",
    "#         self.dropout = Dropout(p=0.5)\n",
    "#         self.out = Linear(256, out_channels) # 256\n",
    "\n",
    "#     def forward(self, x, edge_index, batch, x_feat):\n",
    "#         x = self.gin(x, edge_index, batch)\n",
    "#         x_feat = self.mlp_num(x_feat)\n",
    "#         concat = torch.cat((x, x_feat),dim=1)\n",
    "#         x = self.bn(concat)\n",
    "        \n",
    "#         # Deep & Cross Network\n",
    "#         hl = self.mlp_cross(x)\n",
    "# #         xl = self.crossnet(x)\n",
    "# #         x = torch.cat((xl, hl), dim=1)\n",
    "#         x = self.bn_cat(hl)\n",
    "#         x = self.dropout(x)\n",
    "#         out = self.out(x)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a339c47-6c9b-408d-82ee-77e1efefdebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class DCN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(DCN, self).__init__()\n",
    "#         # Deep & Cross Network\n",
    "#         self.crossnet = CrossNet(in_channels)\n",
    "#         self.mlp_cross = MLP(in_channels, 256, 128)\n",
    "        \n",
    "#         self.bn = BatchNorm1d(128+in_channels)\n",
    "#         self.mlp_cat = MLP(128+in_channels, 512, 256)\n",
    "#         self.bn_out = BatchNorm1d(256)\n",
    "#         self.dropout = Dropout(p=0.5)\n",
    "#         self.out = Linear(256, out_channels) # 256\n",
    "\n",
    "#     def forward(self, x):        \n",
    "#         # Deep & Cross Network\n",
    "#         hl = self.mlp_cross(x)\n",
    "#         xl = self.crossnet(x)\n",
    "#         x = torch.cat((xl, hl), dim=1)\n",
    "#         x = self.bn(x)\n",
    "#         x = self.mlp_cat(x)\n",
    "#         x = self.bn_out(x)\n",
    "# #         x = self.dropout(x)\n",
    "#         out = self.out(x)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "14c8bc7f-07c8-43d0-94c8-3b60f56ce02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_xs, in_xfeats, dim, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.g = SAGE(in_xs, dim, 128) # SAGE\n",
    "        self.mlp = MLP(in_xfeats, dim, 128)\n",
    "        self.lin = Linear(256, 128)\n",
    "        self.lin2 = Linear(128, 128)\n",
    "        self.cross = CrossNet(128, layer_num=2)\n",
    "        self.mlp_cross = MLP(128, 128, 128)\n",
    "        self.mlp_out = MLP(256, 256, 256)\n",
    "        self.out = Linear(256, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, x_feat):\n",
    "        x = self.g(x, edge_index, batch)\n",
    "        x_feat = self.mlp(x_feat)\n",
    "        concat = torch.cat((x, x_feat), dim=1)\n",
    "        x = self.lin(concat)\n",
    "        x = F.dropout(x, p=0.3, training=self.training) # SAGE: 0.3\n",
    "        x = self.lin2(x).relu()\n",
    "        \n",
    "        cross = self.cross(x)\n",
    "        mlp_cross = self.mlp_cross(x)\n",
    "        concat2 = torch.cat((cross, mlp_cross), dim=1)\n",
    "        \n",
    "        x = self.mlp_out(concat2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training) # SAGE: 0.3\n",
    "        out = self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436d70d-f407-4829-bc1a-70ff24aef54d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "711f76cb-8705-4ebf-a0ce-2b21ac50f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "    c=0\n",
    "    correct=0\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, data.x_feat.float())  # Perform a single forward pass. , data.edge_attr.float()\n",
    "    \n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "        c=c+1\n",
    "        correct+=loss.cpu().detach().numpy()\n",
    "    return correct/c\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    c=0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch, data.x_feat.float()) # , data.edge_attr.float()\n",
    "    \n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        correct += loss.cpu().detach().numpy()  # Check against ground-truth labels.\n",
    "        c=c+1\n",
    "    return correct / c  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "08571f39-3903-4f7a-8fa8-769e373c125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features = 9\n",
    "hidden_channels = 32 # SAGE: 32\n",
    "num_feats = 21\n",
    "num_classes = 1\n",
    "num_attr = 3\n",
    "\n",
    "model = Net(num_node_features, num_feats, hidden_channels, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #0.001\n",
    "criterion = torch.nn.L1Loss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.85, patience=3, min_lr=0.000001) #factor=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fbf307aa-531c-49ac-87f3-b48da24513e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "Epoch: 001, Train MAE: 38.7831, Test MAE: 25.8565\n",
      "Epoch: 002, Train MAE: 26.5488, Test MAE: 23.2006\n",
      "Epoch: 003, Train MAE: 24.7791, Test MAE: 25.0603\n",
      "Epoch: 004, Train MAE: 23.9259, Test MAE: 21.6258\n",
      "Epoch: 005, Train MAE: 23.1437, Test MAE: 21.4344\n",
      "Epoch: 006, Train MAE: 22.8809, Test MAE: 26.4973\n",
      "Epoch: 007, Train MAE: 22.5532, Test MAE: 21.8225\n",
      "Epoch: 008, Train MAE: 22.2173, Test MAE: 21.8360\n",
      "Epoch: 009, Train MAE: 22.2246, Test MAE: 21.1542\n",
      "Epoch: 010, Train MAE: 21.9191, Test MAE: 20.6732\n",
      "Epoch: 011, Train MAE: 21.6934, Test MAE: 21.7121\n",
      "Epoch: 012, Train MAE: 21.7322, Test MAE: 20.7380\n",
      "Epoch: 013, Train MAE: 21.8021, Test MAE: 20.6465\n",
      "Epoch: 014, Train MAE: 21.2230, Test MAE: 20.3389\n",
      "Epoch: 015, Train MAE: 21.3750, Test MAE: 20.4680\n",
      "Epoch: 016, Train MAE: 21.2774, Test MAE: 20.3397\n",
      "Epoch: 017, Train MAE: 21.2604, Test MAE: 20.5971\n",
      "Epoch: 018, Train MAE: 21.3211, Test MAE: 19.8427\n",
      "Epoch: 019, Train MAE: 20.9958, Test MAE: 20.3034\n",
      "Epoch: 020, Train MAE: 21.0780, Test MAE: 19.9973\n",
      "Epoch: 021, Train MAE: 20.9158, Test MAE: 20.0853\n",
      "Epoch: 022, Train MAE: 20.9540, Test MAE: 19.9951\n",
      "Epoch: 023, Train MAE: 20.6250, Test MAE: 19.9869\n",
      "Epoch: 024, Train MAE: 20.6282, Test MAE: 19.5382\n",
      "Epoch: 025, Train MAE: 20.4979, Test MAE: 19.5342\n",
      "Epoch: 026, Train MAE: 20.6261, Test MAE: 19.9024\n",
      "Epoch: 027, Train MAE: 20.4450, Test MAE: 20.7575\n",
      "Epoch: 028, Train MAE: 20.4170, Test MAE: 19.9115\n",
      "Epoch: 029, Train MAE: 20.5401, Test MAE: 19.6799\n",
      "Epoch: 030, Train MAE: 20.1922, Test MAE: 19.2698\n",
      "Epoch: 031, Train MAE: 20.0973, Test MAE: 19.4777\n",
      "Epoch: 032, Train MAE: 20.2379, Test MAE: 19.4107\n",
      "Epoch: 033, Train MAE: 19.9576, Test MAE: 19.6898\n",
      "Epoch: 034, Train MAE: 20.0388, Test MAE: 19.1227\n",
      "Epoch: 035, Train MAE: 19.9613, Test MAE: 19.3190\n",
      "Epoch: 036, Train MAE: 20.1724, Test MAE: 19.7853\n",
      "Epoch: 037, Train MAE: 20.0555, Test MAE: 19.8279\n",
      "Epoch: 038, Train MAE: 19.9825, Test MAE: 19.3692\n",
      "Epoch: 039, Train MAE: 19.7600, Test MAE: 19.3980\n",
      "Epoch: 040, Train MAE: 19.6520, Test MAE: 19.2269\n",
      "Epoch: 041, Train MAE: 19.7169, Test MAE: 19.3008\n",
      "Epoch: 042, Train MAE: 19.7251, Test MAE: 19.3954\n",
      "Epoch: 043, Train MAE: 19.5655, Test MAE: 19.0065\n",
      "Epoch: 044, Train MAE: 19.5539, Test MAE: 18.9802\n",
      "Epoch: 045, Train MAE: 19.5643, Test MAE: 19.0712\n",
      "Epoch: 046, Train MAE: 19.4260, Test MAE: 19.5592\n",
      "Epoch: 047, Train MAE: 19.6357, Test MAE: 18.9616\n",
      "Epoch: 048, Train MAE: 19.4090, Test MAE: 18.9796\n",
      "Epoch: 049, Train MAE: 19.4196, Test MAE: 19.0222\n",
      "Epoch: 050, Train MAE: 19.4654, Test MAE: 19.2224\n",
      "Epoch: 051, Train MAE: 19.4456, Test MAE: 19.4344\n",
      "Epoch: 052, Train MAE: 19.3253, Test MAE: 19.0802\n",
      "Epoch: 053, Train MAE: 19.1796, Test MAE: 19.0719\n",
      "Epoch: 054, Train MAE: 19.2304, Test MAE: 18.9041\n",
      "Epoch: 055, Train MAE: 19.2206, Test MAE: 19.7108\n",
      "Epoch: 056, Train MAE: 19.1390, Test MAE: 18.9486\n",
      "Epoch: 057, Train MAE: 19.1597, Test MAE: 19.0096\n",
      "Epoch: 058, Train MAE: 19.1531, Test MAE: 18.9365\n",
      "Epoch: 059, Train MAE: 19.1177, Test MAE: 18.9318\n",
      "Epoch: 060, Train MAE: 19.0318, Test MAE: 18.8019\n",
      "Epoch: 061, Train MAE: 19.1066, Test MAE: 19.0589\n",
      "Epoch: 062, Train MAE: 18.9931, Test MAE: 18.7325\n",
      "Epoch: 063, Train MAE: 19.0664, Test MAE: 19.0504\n",
      "Epoch: 064, Train MAE: 18.9344, Test MAE: 18.8636\n",
      "Epoch: 065, Train MAE: 18.8958, Test MAE: 19.2642\n",
      "Epoch: 066, Train MAE: 18.8785, Test MAE: 18.7032\n",
      "Epoch: 067, Train MAE: 18.8679, Test MAE: 18.7072\n",
      "Epoch: 068, Train MAE: 18.8771, Test MAE: 18.8045\n",
      "Epoch: 069, Train MAE: 18.8547, Test MAE: 18.9554\n",
      "Epoch: 070, Train MAE: 18.8049, Test MAE: 19.2075\n",
      "Epoch: 071, Train MAE: 18.7987, Test MAE: 18.9306\n",
      "Epoch: 072, Train MAE: 18.8136, Test MAE: 18.7812\n",
      "Epoch: 073, Train MAE: 18.7083, Test MAE: 18.6634\n",
      "Epoch: 074, Train MAE: 18.6727, Test MAE: 18.7968\n",
      "Epoch: 075, Train MAE: 18.7511, Test MAE: 19.1377\n",
      "Epoch: 076, Train MAE: 18.7564, Test MAE: 19.0666\n",
      "Epoch: 077, Train MAE: 18.6545, Test MAE: 18.7607\n",
      "Epoch: 078, Train MAE: 18.5959, Test MAE: 19.4592\n",
      "Epoch: 079, Train MAE: 18.6222, Test MAE: 18.7911\n",
      "Epoch: 080, Train MAE: 18.5836, Test MAE: 18.7070\n",
      "Epoch: 081, Train MAE: 18.5018, Test MAE: 18.6869\n",
      "Epoch: 082, Train MAE: 18.4596, Test MAE: 18.8873\n",
      "Epoch: 083, Train MAE: 18.4408, Test MAE: 18.6682\n",
      "Epoch: 084, Train MAE: 18.4950, Test MAE: 18.7401\n",
      "Epoch: 085, Train MAE: 18.4841, Test MAE: 18.8443\n",
      "Epoch: 086, Train MAE: 18.4493, Test MAE: 18.9739\n",
      "Epoch: 087, Train MAE: 18.4044, Test MAE: 18.7747\n",
      "Epoch: 088, Train MAE: 18.2695, Test MAE: 18.6754\n",
      "Epoch: 089, Train MAE: 18.3377, Test MAE: 18.6604\n",
      "Epoch: 090, Train MAE: 18.3587, Test MAE: 18.8507\n",
      "Epoch: 091, Train MAE: 18.2961, Test MAE: 18.5777\n",
      "Epoch: 092, Train MAE: 18.3385, Test MAE: 18.6095\n",
      "Epoch: 093, Train MAE: 18.3745, Test MAE: 18.6767\n",
      "Epoch: 094, Train MAE: 18.3651, Test MAE: 18.7100\n",
      "Epoch: 095, Train MAE: 18.2824, Test MAE: 18.7340\n",
      "Epoch: 096, Train MAE: 18.2174, Test MAE: 18.6881\n",
      "Epoch: 097, Train MAE: 18.2009, Test MAE: 18.5908\n",
      "Epoch: 098, Train MAE: 18.2430, Test MAE: 18.5649\n",
      "Epoch: 099, Train MAE: 18.1860, Test MAE: 18.7697\n",
      "Epoch: 100, Train MAE: 18.2063, Test MAE: 18.8478\n",
      "Epoch: 101, Train MAE: 18.1586, Test MAE: 18.6612\n",
      "Epoch: 102, Train MAE: 18.1589, Test MAE: 18.6450\n",
      "Epoch: 103, Train MAE: 18.0942, Test MAE: 18.6086\n",
      "Epoch: 104, Train MAE: 18.0768, Test MAE: 18.7138\n",
      "Epoch: 105, Train MAE: 18.1132, Test MAE: 18.6673\n",
      "Epoch: 106, Train MAE: 18.1527, Test MAE: 18.5690\n",
      "Epoch: 107, Train MAE: 18.0874, Test MAE: 18.5795\n",
      "Epoch: 108, Train MAE: 18.0685, Test MAE: 18.5919\n",
      "Epoch: 109, Train MAE: 18.1110, Test MAE: 18.6578\n",
      "Epoch: 110, Train MAE: 18.0220, Test MAE: 18.6862\n",
      "Epoch: 111, Train MAE: 17.9839, Test MAE: 18.7073\n",
      "Epoch: 112, Train MAE: 17.9633, Test MAE: 18.5962\n",
      "Epoch: 113, Train MAE: 18.0563, Test MAE: 18.5783\n",
      "Epoch: 114, Train MAE: 18.0259, Test MAE: 18.5440\n",
      "Epoch: 115, Train MAE: 17.9554, Test MAE: 18.5348\n",
      "Epoch: 116, Train MAE: 18.0327, Test MAE: 18.6013\n",
      "Epoch: 117, Train MAE: 17.9831, Test MAE: 18.6200\n",
      "Epoch: 118, Train MAE: 17.9303, Test MAE: 18.5611\n",
      "Epoch: 119, Train MAE: 17.9437, Test MAE: 18.6323\n",
      "Epoch: 120, Train MAE: 17.9828, Test MAE: 18.6403\n",
      "Epoch: 121, Train MAE: 17.8944, Test MAE: 18.5934\n",
      "Epoch: 122, Train MAE: 17.8673, Test MAE: 18.7762\n",
      "Epoch: 123, Train MAE: 17.9126, Test MAE: 18.5424\n",
      "Epoch: 124, Train MAE: 17.8340, Test MAE: 18.5509\n",
      "Epoch: 125, Train MAE: 17.9372, Test MAE: 18.5129\n",
      "Epoch: 126, Train MAE: 17.8923, Test MAE: 18.5675\n",
      "Epoch: 127, Train MAE: 17.8262, Test MAE: 18.6015\n",
      "Epoch: 128, Train MAE: 17.8724, Test MAE: 18.5524\n",
      "Epoch: 129, Train MAE: 17.8039, Test MAE: 18.5212\n",
      "Epoch: 130, Train MAE: 17.7870, Test MAE: 18.5710\n",
      "Epoch: 131, Train MAE: 17.8002, Test MAE: 18.5228\n",
      "Epoch: 132, Train MAE: 17.8332, Test MAE: 18.5066\n",
      "Epoch: 133, Train MAE: 17.8454, Test MAE: 18.5350\n",
      "Epoch: 134, Train MAE: 17.7985, Test MAE: 18.6044\n",
      "Epoch: 135, Train MAE: 17.8342, Test MAE: 18.6282\n",
      "Epoch: 136, Train MAE: 17.8867, Test MAE: 18.5516\n",
      "Epoch: 137, Train MAE: 17.7713, Test MAE: 18.5228\n",
      "Epoch: 138, Train MAE: 17.8131, Test MAE: 18.6009\n",
      "Epoch: 139, Train MAE: 17.7910, Test MAE: 18.5196\n",
      "Epoch: 140, Train MAE: 17.8056, Test MAE: 18.5728\n",
      "Epoch: 141, Train MAE: 17.7575, Test MAE: 18.5761\n",
      "Epoch: 142, Train MAE: 17.6855, Test MAE: 18.5957\n",
      "Epoch: 143, Train MAE: 17.7011, Test MAE: 18.5539\n",
      "Epoch: 144, Train MAE: 17.7095, Test MAE: 18.5436\n",
      "Epoch: 145, Train MAE: 17.7448, Test MAE: 18.5404\n",
      "Epoch: 146, Train MAE: 17.6861, Test MAE: 18.6167\n",
      "Epoch: 147, Train MAE: 17.7141, Test MAE: 18.5820\n",
      "Epoch: 148, Train MAE: 17.7268, Test MAE: 18.5977\n",
      "Epoch: 149, Train MAE: 17.7325, Test MAE: 18.5288\n",
      "Epoch: 150, Train MAE: 17.7112, Test MAE: 18.5744\n",
      "Epoch: 151, Train MAE: 17.7208, Test MAE: 18.5591\n",
      "Epoch: 152, Train MAE: 17.6886, Test MAE: 18.5408\n",
      "Epoch: 153, Train MAE: 17.7420, Test MAE: 18.5358\n",
      "Epoch: 154, Train MAE: 17.6701, Test MAE: 18.5503\n",
      "Epoch: 155, Train MAE: 17.6976, Test MAE: 18.5874\n",
      "Epoch: 156, Train MAE: 17.6779, Test MAE: 18.5574\n",
      "Epoch: 157, Train MAE: 17.6676, Test MAE: 18.5424\n",
      "Epoch: 158, Train MAE: 17.6736, Test MAE: 18.5912\n",
      "Epoch: 159, Train MAE: 17.6570, Test MAE: 18.5464\n",
      "Epoch: 160, Train MAE: 17.6746, Test MAE: 18.5705\n",
      "Epoch: 161, Train MAE: 17.6154, Test MAE: 18.5701\n",
      "Epoch: 162, Train MAE: 17.5815, Test MAE: 18.5620\n",
      "Epoch: 163, Train MAE: 17.6617, Test MAE: 18.5606\n",
      "Epoch: 164, Train MAE: 17.6560, Test MAE: 18.5641\n",
      "Epoch: 165, Train MAE: 17.6544, Test MAE: 18.5708\n",
      "Epoch: 166, Train MAE: 17.6601, Test MAE: 18.5606\n",
      "Epoch: 167, Train MAE: 17.6777, Test MAE: 18.5658\n",
      "Epoch: 168, Train MAE: 17.6647, Test MAE: 18.5607\n",
      "Epoch: 169, Train MAE: 17.6872, Test MAE: 18.5498\n",
      "Epoch: 170, Train MAE: 17.5812, Test MAE: 18.5510\n",
      "Epoch: 171, Train MAE: 17.6740, Test MAE: 18.5494\n",
      "Epoch: 172, Train MAE: 17.6436, Test MAE: 18.5513\n",
      "Epoch: 173, Train MAE: 17.6482, Test MAE: 18.5548\n",
      "Epoch: 174, Train MAE: 17.5989, Test MAE: 18.5464\n",
      "Epoch: 175, Train MAE: 17.6578, Test MAE: 18.5510\n",
      "Epoch: 176, Train MAE: 17.6072, Test MAE: 18.5421\n",
      "Epoch: 177, Train MAE: 17.6294, Test MAE: 18.5580\n",
      "Epoch: 178, Train MAE: 17.6822, Test MAE: 18.5522\n",
      "Epoch: 179, Train MAE: 17.6405, Test MAE: 18.5627\n",
      "Epoch: 180, Train MAE: 17.6608, Test MAE: 18.5744\n",
      "Epoch: 181, Train MAE: 17.6056, Test MAE: 18.5728\n",
      "Epoch: 182, Train MAE: 17.6252, Test MAE: 18.5583\n",
      "Epoch: 183, Train MAE: 17.6316, Test MAE: 18.5457\n",
      "Epoch: 184, Train MAE: 17.6798, Test MAE: 18.5519\n",
      "Epoch: 185, Train MAE: 17.6315, Test MAE: 18.5678\n",
      "Epoch: 186, Train MAE: 17.5792, Test MAE: 18.5470\n",
      "Epoch: 187, Train MAE: 17.6205, Test MAE: 18.5506\n",
      "Epoch: 188, Train MAE: 17.6318, Test MAE: 18.5477\n",
      "Epoch: 189, Train MAE: 17.6192, Test MAE: 18.5530\n",
      "Epoch: 190, Train MAE: 17.5586, Test MAE: 18.5486\n",
      "Epoch: 191, Train MAE: 17.5837, Test MAE: 18.5555\n",
      "Epoch: 192, Train MAE: 17.6265, Test MAE: 18.5532\n",
      "Epoch: 193, Train MAE: 17.6214, Test MAE: 18.5683\n",
      "Epoch: 194, Train MAE: 17.7039, Test MAE: 18.5498\n",
      "Epoch: 195, Train MAE: 17.6294, Test MAE: 18.5510\n",
      "Epoch: 196, Train MAE: 17.6354, Test MAE: 18.5558\n",
      "Epoch: 197, Train MAE: 17.6359, Test MAE: 18.5574\n",
      "Epoch: 198, Train MAE: 17.5987, Test MAE: 18.5476\n",
      "Epoch: 199, Train MAE: 17.5838, Test MAE: 18.5553\n",
      "Epoch: 200, Train MAE: 17.5640, Test MAE: 18.5461\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 200\n",
    "print('start train')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_acc = train(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    train_loss.append(train_acc)\n",
    "    test_loss.append(test_acc)\n",
    "    scheduler.step(test_acc)\n",
    "    print(f'Epoch: {epoch+1:03d}, Train MAE: {train_acc:.4f}, Test MAE: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35131580-625c-463b-ae2d-9d78aec742df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('loss')\n",
    "plt.plot(np.arange(epochs), train_loss, label='train loss')\n",
    "plt.plot(np.arange(epochs), test_loss, label='val loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b497b990-bad4-4a60-a567-4fb855089f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    torch.save(model.state_dict(), \"model/best_numGNN2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea24b96-0bb4-4825-8511-025d23c07178",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2b58b-befe-46af-9ac2-cb889717fceb",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e876ea1-3a3e-4f5d-bfa6-420b19b1c83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 65625\n",
      "Number of test graphs: 16237\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pickle.load(open('data/train/graph_concat.pkl', 'rb'))\n",
    "test_dataset = pickle.load(open('data/test/graph_concat.pkl', 'rb'))\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b15ad756-929a-4341-a850-2443109b32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = []\n",
    "test_feat = []\n",
    "\n",
    "for train_data in train_dataset:\n",
    "    train_feat.append(train_data.x_feat)\n",
    "    \n",
    "for test_data in test_dataset:\n",
    "    test_feat.append(test_data.x_feat)\n",
    "\n",
    "\n",
    "train_feat = torch.cat(train_feat, axis=0)\n",
    "test_feat = torch.cat(test_feat, axis=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_feat = torch.from_numpy(sc.fit_transform(train_feat))\n",
    "test_feat = torch.from_numpy(sc.transform(test_feat))\n",
    "\n",
    "for i, data in enumerate(train_dataset):\n",
    "    data.x_feat = train_feat[i].unsqueeze(0)\n",
    "\n",
    "for i, data in enumerate(test_dataset):\n",
    "    data.x_feat = test_feat[i].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8c5717e-5674-4121-966a-c3babae5999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea2ece4-05e7-469a-a880-0eb27fe99531",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "160f8d28-db63-4202-ab0f-64e030d9b4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (g): SAGE(\n",
       "    (conv1): SAGEConv(9, 32)\n",
       "    (conv2): SAGEConv(32, 32)\n",
       "    (conv3): SAGEConv(32, 32)\n",
       "    (conv4): SAGEConv(32, 32)\n",
       "    (lin): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (lin1): Linear(in_features=21, out_features=32, bias=True)\n",
       "    (lin2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin4): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (lin): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (cross): CrossNet()\n",
       "  (mlp_cross): MLP(\n",
       "    (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp_out): MLP(\n",
       "    (lin1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_node_features = 9\n",
    "hidden_channels = 32\n",
    "num_feats = 21\n",
    "num_classes = 1\n",
    "num_attr = 3\n",
    "\n",
    "model = Net(num_node_features, num_feats, hidden_channels, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #0.001\n",
    "criterion = torch.nn.L1Loss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.85, patience=3, min_lr=0.000001)\n",
    "\n",
    "model.load_state_dict(torch.load('model/best_SAGE_concat.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f3c94-3bb9-44c0-acc4-8e582aeac2d3",
   "metadata": {},
   "source": [
    "## Evaluate Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35b0b9ff-5788-47ec-a798-27ac83cac519",
   "metadata": {},
   "outputs": [],
   "source": [
    "mofname = []\n",
    "co2_select = []\n",
    "\n",
    "for data in test_loader:  # Iterate in batches over the training dataset.\n",
    "    data.to(device)\n",
    "    \n",
    "    out = model(data.x, data.edge_index, data.batch, data.x_feat.float()).cpu().detach().numpy()\n",
    "    mofname.append(data.mofname)\n",
    "    co2_select.append(out)\n",
    "    \n",
    "mofname = np.concatenate(mofname)\n",
    "co2_select = np.concatenate(co2_select).flatten()\n",
    "\n",
    "cut_mof_unit = lambda x: x.split('_')[-1]\n",
    "id_ = np.array(list(map(cut_mof_unit, mofname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9db2345b-cf9e-43c3-bc68-67993c405a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id': id_, 'CO2_working_capacity [mL/g]': co2_select}\n",
    "\n",
    "df_inference = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7435fc60-3757-427b-a8c1-bffe25e4fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgboost = pd.read_csv('xgboost_submission.csv')\n",
    "df_xgboost = df_xgboost.set_index('id')\n",
    "\n",
    "df_xgboost.loc[df_inference.id.values.astype(int)] = np.expand_dims(df_inference['CO2_working_capacity [mL/g]'].values, axis=1)\n",
    "df_xgboost = df_xgboost.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "efb0471b-287b-4ffa-b888-f363e344f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgboost.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1fb8d3ae-2411-4ea2-856f-fe7cd2c5d9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>CO2_working_capacity [mL/g]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68614</td>\n",
       "      <td>173.341354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68615</td>\n",
       "      <td>67.073875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68616</td>\n",
       "      <td>60.989979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68617</td>\n",
       "      <td>56.514645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68618</td>\n",
       "      <td>63.594818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16995</th>\n",
       "      <td>85609</td>\n",
       "      <td>-7.164486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>85610</td>\n",
       "      <td>1.655503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16997</th>\n",
       "      <td>85611</td>\n",
       "      <td>-0.521235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16998</th>\n",
       "      <td>85612</td>\n",
       "      <td>-1.570179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999</th>\n",
       "      <td>85613</td>\n",
       "      <td>-5.174440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  CO2_working_capacity [mL/g]\n",
       "0      68614                   173.341354\n",
       "1      68615                    67.073875\n",
       "2      68616                    60.989979\n",
       "3      68617                    56.514645\n",
       "4      68618                    63.594818\n",
       "...      ...                          ...\n",
       "16995  85609                    -7.164486\n",
       "16996  85610                     1.655503\n",
       "16997  85611                    -0.521235\n",
       "16998  85612                    -1.570179\n",
       "16999  85613                    -5.174440\n",
       "\n",
       "[17000 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccfd22f-5ebe-43cd-9621-06a0ddcd6f37",
   "metadata": {},
   "source": [
    "## Create Latent Space for AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3da5e7a7-88cc-46b5-a7e5-2aca58e7215b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (g): SAGE(\n",
       "    (conv1): SAGEConv(9, 32)\n",
       "    (conv2): SAGEConv(32, 32)\n",
       "    (conv3): SAGEConv(32, 32)\n",
       "    (conv4): SAGEConv(32, 32)\n",
       "    (lin): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (lin1): Linear(in_features=21, out_features=32, bias=True)\n",
       "    (lin2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (lin4): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (lin): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (cross): CrossNet()\n",
       "  (mlp_cross): MLP(\n",
       "    (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (mlp_out): MLP(\n",
       "    (lin1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (lin4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (out): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.out=nn.Sequential(*list(model.out.children())[:-1])\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91628a15-bd6f-4df6-812e-d86634bc8e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 100\n",
      "done: 200\n",
      "done: 300\n",
      "done: 400\n",
      "done: 500\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "    data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch, data.x_feat.float()).cpu().detach().numpy()\n",
    "    x_feat = data.x_feat.cpu().detach().numpy()\n",
    "\n",
    "    out = np.concatenate((out, x_feat), axis=1)\n",
    "    train_x.append(out)\n",
    "    train_y.append(data.y.cpu().detach().numpy())\n",
    "    c=c+1\n",
    "    if(c%100==0):\n",
    "        print('done:',c)\n",
    "\n",
    "train_x = np.concatenate(train_x, axis=0)\n",
    "train_y = np.concatenate(train_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00dc8e64-9c19-401b-864d-13b4539fa5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 100\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "test_x = []\n",
    "test_mofname = []\n",
    "\n",
    "for data in test_loader:  # Iterate in batches over the training dataset.\n",
    "    data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch, data.x_feat.float()).cpu().detach().numpy()\n",
    "    x_feat = data.x_feat.cpu().detach().numpy()\n",
    "\n",
    "    out = np.concatenate((out, x_feat), axis=1)\n",
    "    test_x.append(out)\n",
    "    test_mofname.append(data.mofname)\n",
    "    c=c+1\n",
    "    if(c%100==0):\n",
    "        print('done:',c)\n",
    "\n",
    "test_x = np.concatenate(test_x, axis=0)\n",
    "test_mofname = np.concatenate(test_mofname, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8b951cd-46fd-4d9a-b530-cdf6e6cfc44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_x)\n",
    "test_df = pd.DataFrame(test_x)\n",
    "\n",
    "train_df['target'] = train_y.flatten()\n",
    "test_df['mofname'] = test_mofname.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cf83f50-e2a7-4ab8-8c32-aec32212f8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>mofname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.393690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.533949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.920761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.790710</td>\n",
       "      <td>-0.882568</td>\n",
       "      <td>-0.571121</td>\n",
       "      <td>-0.711980</td>\n",
       "      <td>-0.610755</td>\n",
       "      <td>0.547042</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>0.050804</td>\n",
       "      <td>0.731336</td>\n",
       "      <td>mof_unit_68614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.179619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.036682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.112279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.596504</td>\n",
       "      <td>-0.279894</td>\n",
       "      <td>-0.491427</td>\n",
       "      <td>-0.082455</td>\n",
       "      <td>-0.518308</td>\n",
       "      <td>-0.334211</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.067150</td>\n",
       "      <td>-0.585683</td>\n",
       "      <td>mof_unit_68615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.570940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.469352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.490082</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085405</td>\n",
       "      <td>0.476304</td>\n",
       "      <td>0.123035</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.056075</td>\n",
       "      <td>-0.334211</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.101586</td>\n",
       "      <td>-0.637351</td>\n",
       "      <td>mof_unit_68616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.089923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.092972</td>\n",
       "      <td>...</td>\n",
       "      <td>1.352204</td>\n",
       "      <td>0.873918</td>\n",
       "      <td>0.453466</td>\n",
       "      <td>-0.711980</td>\n",
       "      <td>-0.980542</td>\n",
       "      <td>0.742876</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.123961</td>\n",
       "      <td>-0.594873</td>\n",
       "      <td>mof_unit_68617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.540619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.358086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.436696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736885</td>\n",
       "      <td>0.782988</td>\n",
       "      <td>0.352755</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.333415</td>\n",
       "      <td>-0.138377</td>\n",
       "      <td>0.359778</td>\n",
       "      <td>-0.102154</td>\n",
       "      <td>-0.468214</td>\n",
       "      <td>mof_unit_68618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16232</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.088397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.394831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.958029</td>\n",
       "      <td>3.118466</td>\n",
       "      <td>5.937423</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.795648</td>\n",
       "      <td>-0.627962</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.156696</td>\n",
       "      <td>-1.963071</td>\n",
       "      <td>mof_unit_85609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16233</th>\n",
       "      <td>0.313762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.443608</td>\n",
       "      <td>0.289820</td>\n",
       "      <td>0.350841</td>\n",
       "      <td>0.267429</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714540</td>\n",
       "      <td>2.005436</td>\n",
       "      <td>1.721716</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.980542</td>\n",
       "      <td>-0.921714</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.142419</td>\n",
       "      <td>-1.605460</td>\n",
       "      <td>mof_unit_85610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16234</th>\n",
       "      <td>0.248444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590385</td>\n",
       "      <td>0.052065</td>\n",
       "      <td>0.573007</td>\n",
       "      <td>0.023652</td>\n",
       "      <td>...</td>\n",
       "      <td>1.912784</td>\n",
       "      <td>1.749141</td>\n",
       "      <td>1.644934</td>\n",
       "      <td>-0.711980</td>\n",
       "      <td>-0.425862</td>\n",
       "      <td>0.253291</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.155858</td>\n",
       "      <td>-1.651416</td>\n",
       "      <td>mof_unit_85611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16235</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.856547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.944257</td>\n",
       "      <td>2.093103</td>\n",
       "      <td>2.144743</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.425862</td>\n",
       "      <td>0.449125</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.154843</td>\n",
       "      <td>-1.748159</td>\n",
       "      <td>mof_unit_85612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16236</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.708716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.926558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.436823</td>\n",
       "      <td>2.762963</td>\n",
       "      <td>4.032448</td>\n",
       "      <td>-1.026742</td>\n",
       "      <td>-0.888095</td>\n",
       "      <td>0.644959</td>\n",
       "      <td>-2.779491</td>\n",
       "      <td>-0.158366</td>\n",
       "      <td>-1.946889</td>\n",
       "      <td>mof_unit_85613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16237 rows  278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0    1    2    3    4    5         6          7         8  \\\n",
       "0      28.393690  0.0  0.0  0.0  0.0  0.0  0.000000  30.533949  0.000000   \n",
       "1      10.179619  0.0  0.0  0.0  0.0  0.0  0.000000  11.036682  0.000000   \n",
       "2       9.570940  0.0  0.0  0.0  0.0  0.0  0.000000  10.469352  0.000000   \n",
       "3       8.316006  0.0  0.0  0.0  0.0  0.0  0.000000   9.089923  0.000000   \n",
       "4       9.540619  0.0  0.0  0.0  0.0  0.0  0.000000  10.358086  0.000000   \n",
       "...          ...  ...  ...  ...  ...  ...       ...        ...       ...   \n",
       "16232   0.000000  0.0  0.0  0.0  0.0  0.0  2.088397   0.000000  2.394831   \n",
       "16233   0.313762  0.0  0.0  0.0  0.0  0.0  0.443608   0.289820  0.350841   \n",
       "16234   0.248444  0.0  0.0  0.0  0.0  0.0  0.590385   0.052065  0.573007   \n",
       "16235   0.000000  0.0  0.0  0.0  0.0  0.0  0.856547   0.000000  0.859740   \n",
       "16236   0.000000  0.0  0.0  0.0  0.0  0.0  1.708716   0.000000  1.926558   \n",
       "\n",
       "               9  ...       268       269       270       271       272  \\\n",
       "0      30.920761  ... -0.790710 -0.882568 -0.571121 -0.711980 -0.610755   \n",
       "1      11.112279  ... -0.596504 -0.279894 -0.491427 -0.082455 -0.518308   \n",
       "2      10.490082  ...  1.085405  0.476304  0.123035 -0.397217 -0.056075   \n",
       "3       9.092972  ...  1.352204  0.873918  0.453466 -0.711980 -0.980542   \n",
       "4      10.436696  ...  0.736885  0.782988  0.352755 -0.397217 -0.333415   \n",
       "...          ...  ...       ...       ...       ...       ...       ...   \n",
       "16232   0.000000  ...  2.958029  3.118466  5.937423 -0.397217 -0.795648   \n",
       "16233   0.267429  ...  1.714540  2.005436  1.721716 -0.397217 -0.980542   \n",
       "16234   0.023652  ...  1.912784  1.749141  1.644934 -0.711980 -0.425862   \n",
       "16235   0.000000  ...  1.944257  2.093103  2.144743 -0.397217 -0.425862   \n",
       "16236   0.000000  ...  2.436823  2.762963  4.032448 -1.026742 -0.888095   \n",
       "\n",
       "            273       274       275       276         mofname  \n",
       "0      0.547042  0.359778  0.050804  0.731336  mof_unit_68614  \n",
       "1     -0.334211  0.359778 -0.067150 -0.585683  mof_unit_68615  \n",
       "2     -0.334211  0.359778 -0.101586 -0.637351  mof_unit_68616  \n",
       "3      0.742876  0.359778 -0.123961 -0.594873  mof_unit_68617  \n",
       "4     -0.138377  0.359778 -0.102154 -0.468214  mof_unit_68618  \n",
       "...         ...       ...       ...       ...             ...  \n",
       "16232 -0.627962 -2.779491 -0.156696 -1.963071  mof_unit_85609  \n",
       "16233 -0.921714 -2.779491 -0.142419 -1.605460  mof_unit_85610  \n",
       "16234  0.253291 -2.779491 -0.155858 -1.651416  mof_unit_85611  \n",
       "16235  0.449125 -2.779491 -0.154843 -1.748159  mof_unit_85612  \n",
       "16236  0.644959 -2.779491 -0.158366 -1.946889  mof_unit_85613  \n",
       "\n",
       "[16237 rows x 278 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c59a166-f0e8-438e-95fb-49c5fa3af489",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/train/auto_SAGE_feat_train.csv',index=False)\n",
    "test_df.to_csv('data/test/auto_SAGE_feat_mofname_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12171eea-b02a-49f7-b95d-493d132fec69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMLCC_CUDA",
   "language": "python",
   "name": "tmlcc_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
